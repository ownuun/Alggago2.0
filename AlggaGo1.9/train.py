import os
import time
import re
import numpy as np
import torch
import pygame
import csv 
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from tqdm import tqdm
from opponent_c import model_c_action
from opponent import get_regular_action, get_split_shot_action 

import gymnasium as gym
from gymnasium import spaces

from env import AlggaGoEnv
from physics import WIDTH, HEIGHT, all_stones_stopped, MARGIN

# ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œ import
try:
    from specialized_training_manager import SpecializedTrainingManager
    SPECIALIZED_TRAINING_AVAILABLE = True
except ImportError:
    print("[Warning] ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. specialized_training_manager.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    SPECIALIZED_TRAINING_AVAILABLE = False

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì • ---
MAX_STAGES = 300
TIMESTEPS_PER_STAGE = 50000
SAVE_DIR = "rl_models_competitive"
LOG_DIR = "rl_logs_competitive"
INITIAL_ENT_COEF_A = 0.05
INITIAL_ENT_COEF_B = 0.1
ENT_COEF_INCREMENT = 0.1
MAX_ENT_COEF = 0.5
EVAL_EPISODES_FOR_COMPETITION = 200
GAUNTLET_EVAL_EPISODES_PER_COLOR = 100

# --- ì§„í–‰ë¥  í‘œì‹œ ì½œë°± í´ë˜ìŠ¤ ---
class ProgressCallback(BaseCallback):
    def __init__(self, total_timesteps):
        super().__init__()
        self.total_timesteps = total_timesteps
        self.pbar = None

    def _on_training_start(self):
        # ì‹¤ì œ ëª©í‘œ íƒ€ì„ìŠ¤í…(= learnì— ë„˜ê¸´ total_timesteps)ë¡œ í‘œì‹œ
        self.start_num = self.model.num_timesteps
        self.pbar = tqdm(total=self.total_timesteps,
                         desc="í•™ìŠµ ì§„í–‰ë¥ ",
                         bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')

    def _on_step(self):
        # í˜„ì¬ ì§„í–‰ íƒ€ì„ìŠ¤í… = (ëª¨ë¸ ëˆ„ì ) - (í•™ìŠµ ì‹œì‘ ì‹œì )
        done_ts = self.model.num_timesteps - self.start_num
        # pbar ìœ„ì¹˜ë¥¼ ì§ì ‘ ë§ì¶°ì¤Œ
        if self.pbar:
            self.pbar.n = min(done_ts, self.total_timesteps)
            self.pbar.refresh()
        return True

    def _on_training_end(self):
        if self.pbar: self.pbar.close(); self.pbar = None
# --- Rule-based ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_to_rule_based(model):
    """
    ëª¨ë¸ì˜ ì •ì±…ì„ 'ê·œì¹™ ê¸°ë°˜' í–‰ë™ì„ í•˜ë„ë¡ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    - ì „ëµ: 'ì¼ë°˜ ê³µê²©'ì„ ì••ë„ì ìœ¼ë¡œ ì„ í˜¸í•˜ë„ë¡ ì„¤ì •
    - íŒŒë¼ë¯¸í„°: ê·œì¹™ ê¸°ë°˜ì˜ ê°’ì„ ê·¸ëŒ€ë¡œ ë”°ë¥´ë„ë¡ ì˜¤í”„ì…‹ì„ 0ìœ¼ë¡œ ì„¤ì •
    """
    with torch.no_grad():
        # ì‹ ê²½ë§ì˜ ë§ˆì§€ë§‰ ì¶œë ¥ ë ˆì´ì–´(action_net)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        action_net = model.policy.action_net

        # action_netì˜ ê°€ì¤‘ì¹˜ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬, ì¶œë ¥ì´ í¸í–¥(bias)ì— ì˜í•´ì„œë§Œ ê²°ì •ë˜ë„ë¡ í•©ë‹ˆë‹¤.
        action_net.weight.data.fill_(0.0)

        # ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ì€ 5ê°œì˜ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.
        # [0]: ì¼ë°˜ ê³µê²© ì„ í˜¸ë„ (Regular Attack Preference)
        # [1]: í‹ˆìƒˆ ê³µê²© ì„ í˜¸ë„ (Split Shot Preference)
        # [2]: raw_index (ëŒ ì„ íƒ ì˜¤í”„ì…‹)
        # [3]: raw_angle (ê°ë„ ì˜¤í”„ì…‹)
        # [4]: raw_force (í˜ ì˜¤í”„ì…‹)

        # 1. ì „ëµ ì„ íƒ ì´ˆê¸°í™”
        # 'ì¼ë°˜ ê³µê²©' ì„ í˜¸ë„ëŠ” ë§¤ìš° ë†’ê²Œ, 'í‹ˆìƒˆ ê³µê²©' ì„ í˜¸ë„ëŠ” ë§¤ìš° ë‚®ê²Œ ì„¤ì •
        action_net.bias[0].data.fill_(10.0)  # ì¼ë°˜ ê³µê²© ì„ í˜¸
        action_net.bias[1].data.fill_(-10.0) # í‹ˆìƒˆ ê³µê²© ë¹„ì„ í˜¸

        # 2. íŒŒë¼ë¯¸í„° ì˜¤í”„ì…‹ ì´ˆê¸°í™”
        # ëŒ, ê°ë„, í˜ì— ëŒ€í•œ ìˆ˜ì •ê°’(ì˜¤í”„ì…‹)ì€ ëª¨ë‘ 0ìœ¼ë¡œ ì„¤ì •
        action_net.bias[2].data.fill_(0.0)
        action_net.bias[3].data.fill_(0.0)
        action_net.bias[4].data.fill_(0.0)

        # 3. í–‰ë™ì˜ ë¶„ì‚°(log_std)ì„ ë§¤ìš° ì‘ê²Œ ë§Œë“¤ì–´, ì´ˆê¸° í–‰ë™ì´ ê±°ì˜ ì¼ì •í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
        if isinstance(model.policy.log_std, torch.nn.Parameter):
            model.policy.log_std.data.fill_(-20.0)

# --- ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸ í•¨ìˆ˜ ---
def print_model_parameters(model: PPO):
    print("\n==== ëª¨ë¸ ì´ˆê¸° íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸ ====")
    params = model.get_parameters()['policy']
    for name, tensor in params.items():
        if name in ["mlp_extractor", "value_net"] and isinstance(tensor, dict):
            for sub_name, sub_tensor in tensor.items():
                if hasattr(sub_tensor, 'cpu'):
                    arr = sub_tensor.cpu().numpy()
                    print(f"policy.{name}.{sub_name}: max={np.max(arr):.6f}, min={np.min(arr):.6f}")
        elif hasattr(tensor, 'cpu'):
            arr = tensor.cpu().numpy()
            print(f"policy.{name}: max={np.max(arr):.6f}, min={np.min(arr):.6f}")
    print("=" * 31, "\n")

# --- í™˜ê²½ ìƒì„± í—¬í¼ í•¨ìˆ˜ ---
def make_env_fn():
    def _init():
        env = AlggaGoEnv()
        monitored_env = Monitor(env, filename=LOG_DIR)
        return monitored_env
    return _init

# --- ê¸°íƒ€ í—¬í¼ í•¨ìˆ˜ ---
def clean_models(model_A_path, model_B_path, best_model_paths):
    if not os.path.exists(SAVE_DIR): return
    all_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".zip")]
    to_keep_names = {os.path.basename(p) for p in [model_A_path, model_B_path] if p} | {os.path.basename(p) for p in best_model_paths}
    for fname in all_files:
        if fname in to_keep_names: continue
        try:
            file_to_remove = os.path.join(SAVE_DIR, fname)
            if os.path.exists(file_to_remove): os.remove(file_to_remove)
        except OSError as e: print(f"[WARN] íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
def update_best_models(current_best_models, new_model_path, reward, max_to_keep=5):
    current_best_models.append((new_model_path, reward))
    current_best_models.sort(key=lambda x: x[1], reverse=True)
    return current_best_models[:max_to_keep]
TRAINING_STATE_FILE = os.path.join(SAVE_DIR, "training_state.npy")
def load_training_state():
    if os.path.exists(TRAINING_STATE_FILE):
        try:
            state = np.load(TRAINING_STATE_FILE, allow_pickle=True).item()
            print(f"[INFO] ì´ì „ í•™ìŠµ ìƒíƒœ ë¡œë“œ ì„±ê³µ: {state}")
            return state
        except Exception as e: print(f"[ERROR] í•™ìŠµ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None
def save_training_state(state_dict):
    os.makedirs(SAVE_DIR, exist_ok=True)
    np.save(TRAINING_STATE_FILE, state_dict)
def print_overall_progress(current_stage, total_stages, current_timesteps, total_timesteps):
    stage_progress = (current_stage / total_stages) * 100
    timestep_progress = (current_timesteps / total_timesteps) * 100
    print(f"\n{'='*60}\nğŸ“Š ì „ì²´ í•™ìŠµ ì§„í–‰ë¥ \n   ìŠ¤í…Œì´ì§€: {current_stage}/{total_stages} ({stage_progress:.1f}%)\n   íƒ€ì„ìŠ¤í…: {current_timesteps:,}/{total_timesteps:,} ({timestep_progress:.1f}%)\n{'='*60}")

# --- ê³µì • í‰ê°€(Fair Evaluation) í•¨ìˆ˜ ---
def evaluate_fairly(model_A: PPO, model_B: PPO, num_episodes: int):
    games_per_round = num_episodes // 2
    if games_per_round == 0: return 0.5, 0.5, 0.0, 0.0
    print(f"   - ê³µì •í•œ í‰ê°€: ì´ {num_episodes} ê²Œì„ ({games_per_round} ê²Œì„/ë¼ìš´ë“œ)")

    def _play_round(black_model: PPO, white_model: PPO, num_games: int, round_name: str):
        black_wins = 0
        env = Monitor(AlggaGoEnv())
        for _ in tqdm(range(num_games), desc=round_name, leave=False):
            obs, _ = env.reset(options={"initial_player": "black"})
            done = False
            while not done:
                current_player = env.env.current_player
                action_model = black_model if current_player == 'black' else white_model
                action, _ = action_model.predict(obs, deterministic=True)
                squeezed_action = np.squeeze(action)
                obs, _, done, _, info = env.step(squeezed_action)
            if info.get('winner') == 'black': black_wins += 1
        env.close()
        return black_wins / num_games if num_games > 0 else 0

    # í‰ê°€ ì „ ì›ë˜ ì—”íŠ¸ë¡œí”¼ ì €ì¥ ë° 0ìœ¼ë¡œ ê³ ì •
    original_ent_A = model_A.ent_coef
    original_ent_B = model_B.ent_coef
    win_rate_A, win_rate_B, r1_black_win_rate, r2_black_win_rate = (0.5, 0.5, 0, 0)
    try:
        print("   [INFO] í‰ê°€ë¥¼ ìœ„í•´ ë‘ ëª¨ë¸ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ 0.0ìœ¼ë¡œ ê³ ì •í•©ë‹ˆë‹¤.")
        model_A.ent_coef = 0
        model_B.ent_coef = 0

        r1_black_win_rate = _play_round(model_A, model_B, games_per_round, "1ë¼ìš´ë“œ (Aê°€ í‘ëŒ)")
        print(f"   â–¶ 1ë¼ìš´ë“œ (Model A í‘ëŒ) ìŠ¹ë¥ : {r1_black_win_rate:.2%}")
        r2_black_win_rate = _play_round(model_B, model_A, games_per_round, "2ë¼ìš´ë“œ (Bê°€ í‘ëŒ)")
        print(f"   â–¶ 2ë¼ìš´ë“œ (Model B í‘ëŒ) ìŠ¹ë¥ : {r2_black_win_rate:.2%}")
        win_rate_A = (r1_black_win_rate + (1 - r2_black_win_rate)) / 2
        win_rate_B = (r2_black_win_rate + (1 - r1_black_win_rate)) / 2
    finally:
        # í‰ê°€ í›„ ì›ë˜ ì—”íŠ¸ë¡œí”¼ë¡œ ë³µì›
        model_A.ent_coef = original_ent_A
        model_B.ent_coef = original_ent_B
        print("   [INFO] ì›ë˜ ì—”íŠ¸ë¡œí”¼ ê°’ìœ¼ë¡œ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return win_rate_A, win_rate_B, r1_black_win_rate, r2_black_win_rate

def evaluate_vs_model_c(ppo_model: PPO, num_episodes_per_color: int):
    """PPO ëª¨ë¸ê³¼ ëª¨ë¸ Cì˜ ìŠ¹ë¥  ë° ê° ì „ëµì˜ ì„±ê³µë¥ ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” í•¨ìˆ˜"""
    print(f"   - ëª¨ë¸ Cì™€ íŠ¹ë³„ í‰ê°€: ì´ {num_episodes_per_color * 2} ê²Œì„ (í‘/ë°± ê° {num_episodes_per_color}íŒ)")
    env = AlggaGoEnv()
    from pymunk import Vec2d
    from physics import scale_force, all_stones_stopped

    win_rates = {}
    total_wins = 0
    
    strategy_attempts = {0: 0, 1: 0}
    strategy_successes = {0: 0, 1: 0}
    
    for side in ["black", "white"]:
        ppo_wins_on_side = 0
        desc = f"   PPO({side}) vs C"
        
        for _ in tqdm(range(num_episodes_per_color), desc=desc, leave=False):
            obs, _ = env.reset(options={"initial_player": side})
            done = False; info = {}
            while not done:
                current_player_color = env.current_player
                
                if current_player_color == side:
                    action, _ = ppo_model.predict(obs, deterministic=True)
                    obs, _, done, _, info = env.step(action)
                    
                    strategy = info.get('strategy_choice')
                    if strategy is not None:
                        strategy_attempts[strategy] += 1
                        if info.get('is_regular_success', False) or info.get('is_split_success', False):
                            strategy_successes[strategy] += 1
                else:
                    # ... (ëª¨ë¸ Cì˜ í„´ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼) ...
                    absolute_action = model_c_action(env.stones, current_player_color)
                    if absolute_action:
                        idx, angle, force = absolute_action
                        player_stones = [s for s in env.stones if s.color[:3] == ((0,0,0) if current_player_color=="black" else (255,255,255))]
                        if 0 <= idx < len(player_stones):
                            stone_to_shoot = player_stones[idx]
                            direction = Vec2d(1, 0).rotated(angle)
                            impulse = direction * scale_force(force)
                            stone_to_shoot.body.apply_impulse_at_world_point(impulse, stone_to_shoot.body.position)

                    physics_steps = 0
                    while not all_stones_stopped(env.stones) and physics_steps < 600:
                        env.space.step(1/60.0); physics_steps += 1

                    for shape in env.stones[:]:
                        if not (MARGIN < shape.body.position.x < WIDTH - MARGIN and MARGIN < shape.body.position.y < HEIGHT - MARGIN):
                            env.space.remove(shape, shape.body); env.stones.remove(shape)
                    
                    current_black = sum(1 for s in env.stones if s.color[:3] == (0,0,0))
                    current_white = sum(1 for s in env.stones if s.color[:3] == (255,255,255))
                    if current_black == 0: done = True; info['winner'] = 'white'
                    elif current_white == 0: done = True; info['winner'] = 'black'
                    env.current_player = "white" if current_player_color == "black" else "black"
                    obs = env._get_obs()

            if done and info.get('winner') == side:
                ppo_wins_on_side += 1
        
        win_rate = ppo_wins_on_side / num_episodes_per_color if num_episodes_per_color > 0 else 0
        print(f"   â–¶ PPOê°€ {side}ì¼ ë•Œ ìŠ¹ë¥ : {win_rate:.2%}")
        win_rates[side] = win_rate
        total_wins += ppo_wins_on_side

    env.close()
    
    # --- [âœ… ìµœì¢… ìˆ˜ì •] ëª¨ë“  í†µê³„ ì§€í‘œ ê³„ì‚° ---
    overall_win_rate = total_wins / (num_episodes_per_color * 2) if num_episodes_per_color > 0 else 0
    win_rate_as_black = win_rates.get("black", 0)
    win_rate_as_white = win_rates.get("white", 0)
    
    regular_success_rate = strategy_successes[0] / strategy_attempts[0] if strategy_attempts[0] > 0 else 0
    split_success_rate = strategy_successes[1] / strategy_attempts[1] if strategy_attempts[1] > 0 else 0
    
    total_strategy_attempts = strategy_attempts[0] + strategy_attempts[1]
    regular_attack_ratio = strategy_attempts[0] / total_strategy_attempts if total_strategy_attempts > 0 else 0
    
    # --- ì½˜ì†” ì¶œë ¥ ë¶€ë¶„ ---
    print(f"   â–¶ ëª¨ë¸ PPO ì „ì²´ ìŠ¹ë¥  (vs C): {overall_win_rate:.2%}")
    print(f"   â–¶ ì¼ë°˜ ê³µê²© ì„ íƒ ë¹„ìœ¨: {regular_attack_ratio:.2%}")
    print(f"   â–¶ ì¼ë°˜ ê³µê²© ì„±ê³µë¥ : {regular_success_rate:.2%} ({strategy_successes[0]}/{strategy_attempts[0]})")
    print(f"   â–¶ í‹ˆìƒˆ ê³µê²© ì„±ê³µë¥ : {split_success_rate:.2%} ({strategy_successes[1]}/{strategy_attempts[1]})")
    
    # --- ë°˜í™˜ ê°’ ---
    return (overall_win_rate, win_rate_as_black, win_rate_as_white, 
            regular_success_rate, split_success_rate, regular_attack_ratio)

class VsModelCEnv(gym.Env):
    """
    ë‹¨ì¼ PPO ì—ì´ì „íŠ¸ê°€ ê³ ì • ìƒëŒ€(Model C)ì™€ ë²ˆê°ˆì•„ ì‹¸ìš°ë©° í•™ìŠµí•˜ë„ë¡ ë˜í•‘í•œ í™˜ê²½.
    í•œ ë²ˆì˜ step() í˜¸ì¶œì—ì„œ:
      - PPO(ì—ì´ì „íŠ¸)ì˜ ìˆ˜ë¥¼ env.step(action)ìœ¼ë¡œ ë°˜ì˜
      - ê²Œì„ ë¯¸ì¢…ë£Œë©´, ê³§ë°”ë¡œ Cì˜ ìˆ˜ë¥¼ ë‚´ë¶€ì—ì„œ ì‹¤í–‰
      - ë‹¤ì‹œ PPO ì°¨ë¡€ê°€ ëœ ì‹œì ì˜ ê´€ì¸¡ obs, reward(ì—ì´ì „íŠ¸ ê´€ì ), done ë“±ì„ ë°˜í™˜
    """
    metadata = {"render_modes": []}

    def __init__(self, agent_side="black"):
        super().__init__()
        self.base_env = AlggaGoEnv()  # ê¸°ì¡´ í™˜ê²½ ì¬ì‚¬ìš©
        self.agent_side = agent_side  # 'black' or 'white'
        self.action_space = self.base_env.action_space
        self.observation_space = self.base_env.observation_space

        # ë‚´ë¶€ ìƒíƒœ ì¶”ì ìš©
        self._last_obs = None

    def set_bonus_modes(self, regular_active: bool, split_active: bool):
        """í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ì˜ env_method í˜¸ì¶œì„ ì‹¤ì œ ê²Œì„ í™˜ê²½ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤."""
        self.base_env.set_bonus_modes(regular_active=regular_active, split_active=split_active)

    def reset(self, *, seed=None, options=None):
        # PPOê°€ í•­ìƒ ë¨¼ì € ë‘ë„ë¡ ì‹œì‘ ìƒ‰ì„ ê°•ì œ
        initial_player = self.agent_side
        self._last_obs, info = self.base_env.reset(options={"initial_player": initial_player})
        # í˜¹ì‹œ ì‹œì‘ í”Œë ˆì´ì–´ê°€ PPOê°€ ì•„ë‹Œ ê²½ìš°ì—”, Cê°€ ë¨¼ì € í•œ ìˆ˜ ë‘ê³  PPO ì°¨ë¡€ë¡œ ë§ì¶°ì¤Œ
        if self.base_env.current_player != self.agent_side:
            self._play_model_c_turn()
            self._last_obs = self.base_env._get_obs()
        return self._last_obs, info

    def step(self, action):
        # 1) ì—ì´ì „íŠ¸ ìˆ˜
        obs, reward_agent, terminated, truncated, info = self.base_env.step(action)
        if terminated or truncated:
            return obs, reward_agent, terminated, truncated, info

        # 2) ìƒëŒ€ ìˆ˜ (ëª¨ë¸ C)
        self._play_model_c_turn()

        # 3) ì¢…ë£Œ/íŒ¨ë„í‹° ë³´ì • ë° ë‹¤ìŒ ê´€ì¸¡ ë°˜í™˜
        terminated_now, loser_penalty = self._check_terminal_and_penalty_after_c_turn()
        total_reward = reward_agent + loser_penalty
        self._last_obs = self.base_env._get_obs()
        return self._last_obs, total_reward, terminated_now, False, info

    # ===== ë‚´ë¶€ ìœ í‹¸ =====
    def _play_model_c_turn(self):
        # í˜„ì¬ ì°¨ë¡€ê°€ Cì¸ì§€ í™•ì¸
        current_player_color = self.base_env.current_player
        if current_player_color == self.agent_side:
            return  # ì´ë¯¸ PPO ì°¨ë¡€ë©´ ì•„ë¬´ê²ƒë„ ì•ˆ í•¨

        action_tuple = model_c_action(self.base_env.stones, current_player_color)
        if action_tuple:
            from pymunk import Vec2d
            from physics import scale_force, all_stones_stopped, WIDTH, HEIGHT, MARGIN
            idx, angle, force = action_tuple

            player_color_tuple = (0, 0, 0) if current_player_color == "black" else (255, 255, 255)
            player_stones = [s for s in self.base_env.stones if s.color[:3] == player_color_tuple]
            if 0 <= idx < len(player_stones):
                stone_to_shoot = player_stones[idx]
                direction = Vec2d(1, 0).rotated(angle)
                impulse = direction * scale_force(force)
                stone_to_shoot.body.apply_impulse_at_world_point(impulse, stone_to_shoot.body.position)

            # ë¬¼ë¦¬ ì§„í–‰ (í‰ê°€ ì½”ë“œì™€ ë™ì¼ ìƒí•œ):contentReference[oaicite:2]{index=2}
            from physics import all_stones_stopped, WIDTH, HEIGHT, MARGIN
            physics_steps = 0
            while not all_stones_stopped(self.base_env.stones) and physics_steps < 600:
                self.base_env.space.step(1/60.0)
                physics_steps += 1

            # ë°”ê¹¥ìœ¼ë¡œ ë‚˜ê°„ ëŒ ì œê±°
            for shape in self.base_env.stones[:]:
                x, y = shape.body.position
                if not (MARGIN < x < WIDTH - MARGIN and MARGIN < y < HEIGHT - MARGIN):
                    if shape in self.base_env.space.shapes:
                        self.base_env.space.remove(shape, shape.body)
                    if shape in self.base_env.stones:
                        self.base_env.stones.remove(shape)

            # í„´ ì „í™˜ ë° ê´€ì¸¡ ì—…ë°ì´íŠ¸
            self.base_env.current_player = "white" if current_player_color == "black" else "black"

    def _check_terminal_and_penalty_after_c_turn(self):
        """
        C ì°¨ë¡€ ì§„í–‰ ì§í›„ ì¢…ë£Œ ì—¬ë¶€ì™€ ì—ì´ì „íŠ¸ ê´€ì  íŒ¨ë„í‹°ë¥¼ ê³„ì‚°.
        env.step ë‚´ë¶€ì˜ ë³´ìƒì€ 'ìˆ˜ë¥¼ ë‘” ìª½' ê¸°ì¤€ìœ¼ë¡œ ì‚°ì¶œë˜ë¯€ë¡œ,
        Cê°€ ì´ê²¨ì„œ ëë‚œ ê²½ìš° ì—ì´ì „íŠ¸ì— ì•½í•œ íŒ¨ë„í‹°ë¥¼ ë”í•´ì¤Œ(-5.0).
        """
        current_black = sum(1 for s in self.base_env.stones if s.color[:3] == (0, 0, 0))
        current_white = sum(1 for s in self.base_env.stones if s.color[:3] == (255, 255, 255))
        if current_black == 0 and current_white > 0:
            # ë°± ìŠ¹(í‘ ì˜¬ì•„ì›ƒ)
            winner = "white"
            terminated = True
        elif current_white == 0 and current_black > 0:
            winner = "black"
            terminated = True
        else:
            return False, 0.0

        # ì—ì´ì „íŠ¸ íŒ¨ë°° ì‹œë§Œ ì‘ì€ íŒ¨ë„í‹°
        agent_color = self.agent_side
        if (winner == "white" and agent_color == "black") or (winner == "black" and agent_color == "white"):
            return True, -5.0
        return True, 0.0

class VsFixedOpponentEnv(gym.Env):
    """
    ë‹¨ì¼ PPO ì—ì´ì „íŠ¸ê°€ 'ê³ ì •ëœ PPO ìƒëŒ€(opponent_model)'ì™€ ë²ˆê°ˆì•„ ì‹¸ìš°ë©° í•™ìŠµí•˜ëŠ” í™˜ê²½.
    """
    metadata = {"render_modes": []}

    def __init__(self, opponent_model: PPO, agent_side="black"):
        super().__init__()
        self.base_env = AlggaGoEnv()
        self.opponent = opponent_model
        self.agent_side = agent_side  # 'black' or 'white'
        self.action_space = self.base_env.action_space
        self.observation_space = self.base_env.observation_space
        self._last_obs = None

    def reset(self, *, seed=None, options=None):
        initial_player = self.agent_side
        self._last_obs, info = self.base_env.reset(options={"initial_player": initial_player})
        # ì‹œì‘ ì°¨ë¡€ê°€ ì—ì´ì „íŠ¸ê°€ ì•„ë‹ˆë©´, ìƒëŒ€ê°€ ë¨¼ì € í•œ ìˆ˜ ë‘ê³  ì—ì´ì „íŠ¸ ì°¨ë¡€ë¡œ ë§ì¶¤
        if self.base_env.current_player != self.agent_side:
            self._play_opponent_turn()
            self._last_obs = self.base_env._get_obs()
        return self._last_obs, info

    def step(self, action):
        # 1) ì—ì´ì „íŠ¸ ìˆ˜
        obs, reward_agent, terminated, truncated, info = self.base_env.step(action)
        if terminated or truncated:
            return obs, reward_agent, terminated, truncated, info

        # 2) ìƒëŒ€ ìˆ˜ (PPO)
        # [âœ… ìµœì¢… ìˆ˜ì •] í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì„ opp_reward ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤.
        opp_reward = self._play_opponent_turn()

        # 3) ì¢…ë£Œ/íŒ¨ë„í‹° ë³´ì • ë° ë‹¤ìŒ ê´€ì¸¡ ë°˜í™˜ (ë‹¤ì‹œ ì—ì´ì „íŠ¸ ì°¨ë¡€)
        terminated_now, loser_penalty = self._check_terminal_and_penalty_after_opponent()
        total_reward = (reward_agent - opp_reward) + loser_penalty
        self._last_obs = self.base_env._get_obs()
        return self._last_obs, total_reward, terminated_now, False, info

    # ===== ë‚´ë¶€ ìœ í‹¸ =====
    def _play_opponent_turn(self):
        if self.base_env.current_player == self.agent_side:
            return 0.0  # ìƒëŒ€ í„´ì´ ì•„ë‹ˆë©´ ë³´ìƒ 0 ë°˜í™˜
        opp_obs = self.base_env._get_obs()
        opp_action, _ = self.opponent.predict(opp_obs, deterministic=True)
        # ìƒëŒ€ë°©ì˜ step ê²°ê³¼ì—ì„œ reward ê°’ì„ ë°›ì•„ì˜´
        _obs, opp_reward, _terminated, _truncated, _info = self.base_env.step(opp_action)
        return opp_reward
    
    def set_opponent(self, new_opponent_model: PPO):
        self.opponent = new_opponent_model

    def _check_terminal_and_penalty_after_opponent(self):
        current_black = sum(1 for s in self.base_env.stones if s.color[:3] == (0, 0, 0))
        current_white = sum(1 for s in self.base_env.stones if s.color[:3] == (255, 255, 255))
        if current_black == 0 and current_white > 0:
            winner = "white"; terminated = True
        elif current_white == 0 and current_black > 0:
            winner = "black"; terminated = True
        else:
            return False, 0.0

        # ì—ì´ì „íŠ¸ê°€ ì§„ ê²½ìš°ë§Œ ì‘ì€ íŒ¨ë„í‹°
        if (winner == "white" and self.agent_side == "black") or \
           (winner == "black" and self.agent_side == "white"):
            return True, -5.0
        return True, 0.0
    
# --- Vs Model C í™˜ê²½ ìƒì„± í—¬í¼ ---
def make_vs_c_env_vec(n_envs: int = 2):
    """
    PPOê°€ Model Cì™€ ë²ˆê°ˆì•„ ì‹¸ìš°ë©° í•™ìŠµí•˜ë„ë¡ í‘/ë°±ì„ ì„ì€ VecEnvë¥¼ ë§Œë“­ë‹ˆë‹¤.
    ì§ìˆ˜ index -> black, í™€ìˆ˜ index -> white
    """
    def _maker(i):
        side = "black" if i % 2 == 0 else "white"
        return lambda: VsModelCEnv(agent_side=side)
    return DummyVecEnv([_maker(i) for i in range(n_envs)])

def make_vs_opponent_env_vec(opponent_model: PPO, n_envs: int = 2):
    """
    ë³‘ë ¬ env ì¤‘ ì§ìˆ˜ indexëŠ” agent=í‘, í™€ìˆ˜ indexëŠ” agent=ë°±ìœ¼ë¡œ ë§Œë“¤ì–´
    í•™ìŠµ ê³¼ì •ì—ì„œ ìƒ‰ìƒì´ ê· í˜•ë˜ë„ë¡ í•¨.
    """
    def _maker(i):
        side = "black" if i % 2 == 0 else "white"
        return lambda: VsFixedOpponentEnv(opponent_model=opponent_model, agent_side=side)
    return DummyVecEnv([_maker(i) for i in range(n_envs)])

def train_vs_model_c(total_timesteps=100_000, agent_side="black", ent_coef=0.1, save_name="ppo_vs_c"):
    """
    PPO í•˜ë‚˜ë¥¼ ê³ ì • ìƒëŒ€(Model C)ì™€ ì‹¸ìš°ë©° í•™ìŠµí•˜ëŠ” ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„.
    """
    os.makedirs(SAVE_DIR, exist_ok=True)
    os.makedirs(LOG_DIR, exist_ok=True)

    # ë˜í¼ í™˜ê²½ ì‚¬ìš©
    def _make():
        return VsModelCEnv(agent_side=agent_side)
    env = DummyVecEnv([_make])

    model = PPO("MlpPolicy", env, verbose=1, ent_coef=ent_coef)
    # (ì„ íƒ) ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ì´ˆê¸°í™” ì¬ì‚¬ìš© ê°€ëŠ¥:contentReference[oaicite:4]{index=4}
    initialize_to_rule_based(model)

    print(f"[INFO] PPO vs Model C í•™ìŠµ ì‹œì‘: total_timesteps={total_timesteps}, side={agent_side}, ent_coef={ent_coef}")
    model.learn(total_timesteps=total_timesteps, callback=ProgressCallback(total_timesteps), reset_num_timesteps=False)

    save_path = os.path.join(SAVE_DIR, f"{save_name}_{agent_side}_{total_timesteps}.zip")
    model.save(save_path)
    print(f"[INFO] í•™ìŠµ ì™„ë£Œ. ì €ì¥: {os.path.basename(save_path)}")
    return model, save_path

# --- ì‹œê°í™” í•¨ìˆ˜ ---
def visualize_one_game(model_A: PPO, model_B: PPO, ent_A: float, ent_B: float, stage_num: int, force_A_as_black: bool = None):
    """
    í•œ ê²Œì„ì„ ì‹œê°í™”í•©ë‹ˆë‹¤. (ìˆ˜ì •ëœ ë²„ì „: PPO í„´ì˜ ë¬¼ë¦¬ ê³¼ì •ì„ í”„ë ˆì„ë³„ë¡œ ë Œë”ë§)
    force_A_as_black: Trueì´ë©´ Aê°€ í‘ëŒ, Falseì´ë©´ Bê°€ í‘ëŒ, Noneì´ë©´ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.
    """
    stage_str = f"ìŠ¤í…Œì´ì§€ {stage_num}" if stage_num > 0 else "ì´ˆê¸° ìƒíƒœ"
    
    if force_A_as_black is True:
        black_model, white_model = model_A, model_B
        caption = f"{stage_str} Eval: A(Black, ent={ent_A:.3f}) vs B(White, ent={ent_B:.3f})"
    elif force_A_as_black is False:
        black_model, white_model = model_B, model_A
        caption = f"{stage_str} Eval: B(Black, ent={ent_B:.3f}) vs A(White, ent={ent_A:.3f})"
    else:
        if ent_A >= ent_B:
            black_model, white_model = model_A, model_B
            caption = f"{stage_str} Eval: A(Black, ent={ent_A:.3f}) vs B(White, ent={ent_B:.3f})"
        else:
            black_model, white_model = model_B, model_A
            caption = f"{stage_str} Eval: B(Black, ent={ent_B:.3f}) vs A(White, ent={ent_A:.3f})"

    print(f"\n--- ì‹œê°í™” í‰ê°€: {caption} ---")
    
    env = AlggaGoEnv()
    obs, _ = env.reset(options={"initial_player": "black"})
    pygame.init()
    screen = pygame.display.set_mode((WIDTH, HEIGHT))
    pygame.display.set_caption(caption)

    from pymunk import Vec2d
    from physics import scale_force

    done = False
    info = {}
    
    while not done:
        for event in pygame.event.get():
            if event.type == pygame.QUIT: done = True
        if done: break

        current_player = env.current_player
        action_model = black_model if current_player == "black" else white_model

        env.render(screen=screen)
        pygame.display.flip()
        time.sleep(0.5)

        action_values, _ = action_model.predict(obs, deterministic=True)
        
        player_color_tuple = (0,0,0) if env.current_player == "black" else (255,255,255)
        player_stones = [s for s in env.stones if s.color[:3] == player_color_tuple]
        opponent_stones = [s for s in env.stones if s.color[:3] != player_color_tuple]

        if not player_stones or not opponent_stones:
            done = True; continue

        if len(opponent_stones) < 2:
            strategy_choice = 0
        else:
            strategy_preferences = np.asarray(action_values[:2], dtype=np.float32)
            max_pref = float(np.max(strategy_preferences)); exp_p = np.exp(strategy_preferences - max_pref)
            probs = exp_p / (np.sum(exp_p) + 1e-8)
            strategy_choice = int(np.random.choice(2, p=probs)) if np.all(np.isfinite(probs)) and probs.sum() > 0 else int(np.argmax(strategy_preferences))
            
        chosen_str = 'ì¼ë°˜ê³µê²©(0)' if strategy_choice == 0 else 'ìŠ¤í”Œë¦¿ìƒ·(1)'
        print(f"[viz] {current_player} í„´ ì „ëµ: {chosen_str}")

        rule_action = get_split_shot_action(player_stones, opponent_stones) if strategy_choice == 1 else get_regular_action(player_stones, opponent_stones)
        if rule_action is None: rule_action = get_regular_action(player_stones, opponent_stones)

        if rule_action:
            raw_index, raw_angle, raw_force = action_values[2:]
            raw_idx_val, raw_angle_val, raw_force_val = action_values[2:]
            raw_index = np.clip(raw_idx_val, -1.0, 1.0)
            raw_angle = np.clip(raw_angle_val, -1.0, 1.0)
            raw_force = np.clip(raw_force_val, -1.0, 1.0)

            index_weight = raw_index * env.exploration_range['index']
            angle_offset = raw_angle * env.exploration_range['angle']
            force_offset = raw_force * env.exploration_range['force']
            rule_idx, rule_angle, rule_force = rule_action
            
            final_idx = np.clip(rule_idx + int(np.round(index_weight)), 0, len(player_stones)-1) if len(player_stones) > 1 else 0
            final_angle = rule_angle + angle_offset
            final_force = np.clip(rule_force + force_offset, 0.0, 1.0)
            
            selected_stone_to_shoot = player_stones[final_idx]
            direction = Vec2d(1, 0).rotated(final_angle)
            impulse = direction * scale_force(final_force)
            selected_stone_to_shoot.body.apply_impulse_at_world_point(impulse, selected_stone_to_shoot.body.position)

            physics_steps = 0
            while not all_stones_stopped(env.stones) and physics_steps < 600:
                env.space.step(1/60.0); env.render(screen=screen)
                pygame.display.flip(); pygame.time.delay(16)
                physics_steps += 1
        
        for shape in env.stones[:]:
            if not (MARGIN < shape.body.position.x < WIDTH - MARGIN and MARGIN < shape.body.position.y < HEIGHT - MARGIN):
                if shape in env.space.shapes: env.space.remove(shape, shape.body)
                if shape in env.stones: env.stones.remove(shape)

        current_black = sum(1 for s in env.stones if s.color[:3] == (0,0,0))
        current_white = sum(1 for s in env.stones if s.color[:3] == (255,255,255))
        
        if current_black == 0: done = True; info['winner'] = 'white'
        elif current_white == 0: done = True; info['winner'] = 'black'

        if not done: env.current_player = "white" if env.current_player == "black" else "black"
        obs = env._get_obs()

    winner = info.get('winner', 'Draw/Timeout')
    print(f">>> ì‹œê°í™” ì¢…ë£Œ: ìµœì¢… ìŠ¹ì {winner} <<<")
    time.sleep(2)
    pygame.quit()


def visualize_vs_model_c(ppo_model: PPO, round_num: int, ppo_player_side: str):
    """
    PPO ëª¨ë¸ê³¼ ëª¨ë¸ Cì˜ ëŒ€ê²°ì„ ì‹œê°í™”í•©ë‹ˆë‹¤. (ìˆ˜ì •ëœ ë²„ì „: PPO í„´ ë Œë”ë§ í¬í•¨)
    ppo_player_side: PPO ëª¨ë¸ì´ í”Œë ˆì´í•  ìƒ‰ìƒ ('black' ë˜ëŠ” 'white')
    """
    stage_str = f"íŠ¹ë³„ í›ˆë ¨ {round_num}ë¼ìš´ë“œ"
    caption = (f"{stage_str}: ëª¨ë¸ A(í‘ëŒ) vs ëª¨ë¸ C(ë°±ëŒ)" if ppo_player_side == "black"
               else f"{stage_str}: ëª¨ë¸ C(í‘ëŒ) vs ëª¨ë¸ A(ë°±ëŒ)")

    print(f"\n--- {stage_str} ì‹œê°í™” ({caption}) ---")

    env = AlggaGoEnv()
    obs, _ = env.reset(options={"initial_player": "black"})
    
    pygame.init()
    screen = pygame.display.set_mode((WIDTH, HEIGHT))
    pygame.display.set_caption(caption)

    from pymunk import Vec2d
    from physics import scale_force, all_stones_stopped

    done = False
    info = {}
    while not done:
        for event in pygame.event.get():
            if event.type == pygame.QUIT: done = True
        if done: continue

        current_player_color = env.current_player

        env.render(screen=screen)
        pygame.display.flip()
        time.sleep(0.5)

        if current_player_color == ppo_player_side:
            action_values, _ = ppo_model.predict(obs, deterministic=True)
            
            player_color_tuple = (0,0,0) if env.current_player == "black" else (255,255,255)
            player_stones = [s for s in env.stones if s.color[:3] == player_color_tuple]
            opponent_stones = [s for s in env.stones if s.color[:3] != player_color_tuple]

            if not player_stones or not opponent_stones:
                done = True; continue

            if len(opponent_stones) < 2:
                strategy_choice = 0
            else:
                strategy_preferences = np.asarray(action_values[:2], dtype=np.float32)
                max_pref = float(np.max(strategy_preferences)); exp_p = np.exp(strategy_preferences - max_pref)
                probs = exp_p / (np.sum(exp_p) + 1e-8)
                strategy_choice = int(np.random.choice(2, p=probs)) if np.all(np.isfinite(probs)) and probs.sum() > 0 else int(np.argmax(strategy_preferences))
            chosen_str = 'ì¼ë°˜ê³µê²©(0)' if strategy_choice == 0 else 'ìŠ¤í”Œë¦¿ìƒ·(1)'
            print(f"[viz-vsC] PPO í„´ ì „ëµ: {chosen_str}")

            rule_action = get_split_shot_action(player_stones, opponent_stones) if strategy_choice == 1 else get_regular_action(player_stones, opponent_stones)
            if rule_action is None: rule_action = get_regular_action(player_stones, opponent_stones)

            if rule_action:
                raw_index, raw_angle, raw_force = action_values[2:]

                raw_idx_val, raw_angle_val, raw_force_val = action_values[2:]
                raw_index = np.clip(raw_idx_val, -1.0, 1.0)
                raw_angle = np.clip(raw_angle_val, -1.0, 1.0)
                raw_force = np.clip(raw_force_val, -1.0, 1.0)

                index_weight = raw_index * env.exploration_range['index']
                angle_offset = raw_angle * env.exploration_range['angle']
                force_offset = raw_force * env.exploration_range['force']
                rule_idx, rule_angle, rule_force = rule_action
                
                final_idx = np.clip(rule_idx + int(np.round(index_weight)), 0, len(player_stones)-1) if len(player_stones) > 1 else 0
                final_angle = rule_angle + angle_offset
                final_force = np.clip(rule_force + force_offset, 0.0, 1.0)
                
                selected_stone_to_shoot = player_stones[final_idx]
                direction = Vec2d(1, 0).rotated(final_angle)
                impulse = direction * scale_force(final_force)
                selected_stone_to_shoot.body.apply_impulse_at_world_point(impulse, selected_stone_to_shoot.body.position)
        else:
            action_tuple = model_c_action(env.stones, current_player_color)
            if action_tuple:
                idx, angle, force = action_tuple
                player_color_tuple = (0, 0, 0) if current_player_color == "black" else (255, 255, 255)
                player_stones = [s for s in env.stones if s.color[:3] == player_color_tuple]
                if 0 <= idx < len(player_stones):
                    stone_to_shoot = player_stones[idx]
                    direction = Vec2d(1, 0).rotated(angle)
                    impulse = direction * scale_force(force)
                    stone_to_shoot.body.apply_impulse_at_world_point(impulse, stone_to_shoot.body.position)

        physics_steps = 0
        while not all_stones_stopped(env.stones) and physics_steps < 600:
            env.space.step(1/60.0); env.render(screen=screen)
            pygame.display.flip(); pygame.time.delay(16)
            physics_steps += 1

        for shape in env.stones[:]:
            if not (MARGIN < shape.body.position.x < WIDTH - MARGIN and MARGIN < shape.body.position.y < HEIGHT - MARGIN):
                if shape in env.space.shapes: env.space.remove(shape, shape.body)
                if shape in env.stones: env.stones.remove(shape)

        current_black = sum(1 for s in env.stones if s.color[:3] == (0, 0, 0))
        current_white = sum(1 for s in env.stones if s.color[:3] == (255, 255, 255))
        if current_black == 0: done = True; info['winner'] = 'white'
        elif current_white == 0: done = True; info['winner'] = 'black'

        if not done: env.current_player = "white" if current_player_color == "black" else "black"
        obs = env._get_obs()

    winner = info.get('winner', 'Draw/Timeout')
    print(f">>> ì‹œê°í™” ì¢…ë£Œ: ìµœì¢… ìŠ¹ì {winner} <<<")
    time.sleep(2)
    pygame.quit()


def visualize_split_shot_debug(model: PPO):
    """
    'í‹ˆìƒˆ ê³µê²©' ì „ëµë§Œ ì‚¬ìš©í•˜ì—¬ í•œ ê²Œì„ì„ ì‹œê°í™”í•˜ê³ , ë§¤ í„´ì˜ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ë¥¼ í„°ë¯¸ë„ì— ì¶œë ¥í•˜ëŠ”
    ë””ë²„ê¹… ì „ìš© í•¨ìˆ˜.
    """
    print("\n" + "="*50)
    print("ğŸ”¬      'í‹ˆìƒˆ ê³µê²©' ë””ë²„ê¹… ì‹œê°í™” ì‹œì‘      ğŸ”¬")
    print("="*50)

    env = AlggaGoEnv()
    obs, _ = env.reset(options={"initial_player": "black"})
    
    pygame.init()
    screen = pygame.display.set_mode((WIDTH, HEIGHT))
    pygame.display.set_caption("DEBUG: Split Shot Only")

    from pymunk import Vec2d
    from physics import scale_force, all_stones_stopped
    import itertools

    done = False
    info = {}
    while not done:
        for event in pygame.event.get():
            if event.type == pygame.QUIT: done = True
        if done: continue

        current_player_color = env.current_player
        env.render(screen=screen)
        pygame.display.flip()
        time.sleep(1.0)

        if current_player_color == "black": # PPO ëª¨ë¸ì˜ í„´ (í‘ëŒ ê³ ì •)
            action_values, _ = model.predict(obs, deterministic=True)
            
            player_stones = [s for s in env.stones if s.color[:3] == (0,0,0)]
            opponent_stones = [s for s in env.stones if s.color[:3] == (255,255,255)]

            if not player_stones or not opponent_stones:
                done = True; continue

            strategy_choice = 1
            print(f"\n[DEBUG] PPO í„´: 'í‹ˆìƒˆ ê³µê²©' ê°•ì œ ì‹¤í–‰")

            rule_action = get_split_shot_action(player_stones, opponent_stones)
            if rule_action is None:
                print("[DEBUG] í‹ˆìƒˆ ê³µê²© ê°€ëŠ¥í•œ ìˆ˜ê°€ ì—†ì–´ í„´ì„ ë„˜ê¹ë‹ˆë‹¤.")
                env.current_player = "white"
                obs = env._get_obs()
                continue
            
            raw_idx_val, raw_angle_val, raw_force_val = action_values[2:]
            raw_index = np.clip(raw_idx_val, -1.0, 1.0)
            raw_angle = np.clip(raw_angle_val, -1.0, 1.0)
            raw_force = np.clip(raw_force_val, -1.0, 1.0)

            index_weight = raw_index * env.exploration_range['index']
            angle_offset = raw_angle * env.exploration_range['angle']
            force_offset = raw_force * env.exploration_range['force']
            rule_idx, rule_angle, rule_force = rule_action
            
            final_idx = np.clip(rule_idx + int(np.round(index_weight)), 0, len(player_stones)-1)
            final_angle = rule_angle + angle_offset
            final_force = np.clip(rule_force + force_offset, 0.0, 1.0)
            
            selected_stone_to_shoot = player_stones[final_idx]
            
            direction = Vec2d(1, 0).rotated(final_angle)
            impulse = direction * scale_force(final_force)
            selected_stone_to_shoot.body.apply_impulse_at_world_point(impulse, selected_stone_to_shoot.body.position)

            physics_steps = 0
            while not all_stones_stopped(env.stones) and physics_steps < 600:
                env.space.step(1/60.0); env.render(screen=screen)
                pygame.display.flip(); pygame.time.delay(16)
                physics_steps += 1

            moved_stone_final_pos = selected_stone_to_shoot.body.position
            opponent_stones_after_shot = [s for s in env.stones if s.color[:3] == (255,255,255)]
            
            max_wedge_reward = 0.0
            if len(opponent_stones_after_shot) >= 2:
                for o1, o2 in itertools.combinations(opponent_stones_after_shot, 2):
                    p1, p2 = o1.body.position, o2.body.position
                    p3 = moved_stone_final_pos
                    v = p2 - p1; w = p3 - p1
                    t = w.dot(v) / (v.dot(v) + 1e-6)
                    if 0 < t < 1:
                        dist_to_segment = (p3 - (p1 + t * v)).length
                        dist_between_opponents = (p1 - p2).length

                        # [âœ… ìµœì¢… ì‚­ì œ] 'ë„ˆë¬´ ë„“ì€ í‹ˆìƒˆëŠ” ë¬´ì‹œ'í•˜ëŠ” ì¡°ê±´ë¬¸ì„ ì‚­ì œí•©ë‹ˆë‹¤.
                        wedge_threshold = dist_between_opponents * 0.15
                        if dist_to_segment < wedge_threshold:
                            current_reward = (1 - (dist_to_segment / wedge_threshold)) * 0.5 
                            if current_reward > max_wedge_reward:
                                max_wedge_reward = current_reward

            if max_wedge_reward > 0:
                print(f"   [DEBUG] >> ê²°ê³¼: í‹ˆìƒˆ ê³µê²© ì„±ê³µ! (ë³´ìƒ: {max_wedge_reward:.2f})")
            else:
                print(f"   [DEBUG] >> ê²°ê³¼: í‹ˆìƒˆ ê³µê²© ì‹¤íŒ¨.")

        else: # ëª¨ë¸ Cì˜ í„´
            print(f"\n[DEBUG] ëª¨ë¸ C í„´...")
            action_tuple = model_c_action(env.stones, current_player_color)
            if action_tuple:
                idx, angle, force = action_tuple
                player_stones_c = [s for s in env.stones if s.color[:3] == (255,255,255)]
                if 0 <= idx < len(player_stones_c):
                    stone_to_shoot_c = player_stones_c[idx]
                    direction_c = Vec2d(1, 0).rotated(angle)
                    impulse_c = direction_c * scale_force(force)
                    stone_to_shoot_c.body.apply_impulse_at_world_point(impulse_c, stone_to_shoot_c.body.position)
            
            physics_steps_c = 0
            while not all_stones_stopped(env.stones) and physics_steps_c < 600:
                env.space.step(1/60.0); env.render(screen=screen)
                pygame.display.flip(); pygame.time.delay(16)
                physics_steps_c += 1

        for shape in env.stones[:]:
            if not (MARGIN < shape.body.position.x < WIDTH - MARGIN and MARGIN < shape.body.position.y < HEIGHT - MARGIN):
                if shape in env.space.shapes: env.space.remove(shape, shape.body)
                if shape in env.stones: env.stones.remove(shape)

        current_black = sum(1 for s in env.stones if s.color[:3] == (0, 0, 0))
        current_white = sum(1 for s in env.stones if s.color[:3] == (255, 255, 255))
        if current_black == 0: done = True; info['winner'] = 'white'
        elif current_white == 0: done = True; info['winner'] = 'black'

        if not done: env.current_player = "white" if current_player_color == "black" else "black"
        obs = env._get_obs()

    winner = info.get('winner', 'Draw/Timeout')
    print(f"\n>>> ë””ë²„ê¹… ì‹œê°í™” ì¢…ë£Œ: ìµœì¢… ìŠ¹ì {winner} <<<")
    time.sleep(3)
    pygame.quit()

def run_gauntlet_training(model_to_train, model_name, initial_timesteps):
    """
    ì£¼ì–´ì§„ ëª¨ë¸ì´ ëª¨ë¸ Cë¥¼ ì´ê¸¸ ë•Œê¹Œì§€ í›ˆë ¨í•˜ëŠ” ì˜ˆì„ ì „ í•¨ìˆ˜. (ìµœì¢… ë²„ì „)
    """
    print("\n" + "="*50)
    print(f"ğŸ¥Š      íŠ¹ë³„ ì˜ˆì„  ì‹œì‘: ëª¨ë¸ {model_name} vs ëª¨ë¸ C       ğŸ¥Š")
    print("="*50)

    GAUNTLET_LOG_FILE = os.path.join(LOG_DIR, "gauntlet_log.csv")
    if not os.path.exists(GAUNTLET_LOG_FILE):
        with open(GAUNTLET_LOG_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Round", "Total Timesteps", "Win Rate as Black", "Win Rate as White", "Overall Win Rate", "Regular Success", "Split Success", "Regular Ratio"])

    GAUNTLET_SAVE_PATH = os.path.join(SAVE_DIR, f"model_{model_name.lower()}_gauntlet_in_progress.zip")
    
    N_ENVS_VS_C = 2 
    gauntlet_env_raw = make_vs_c_env_vec(n_envs=N_ENVS_VS_C)
    gauntlet_env = VecNormalize(gauntlet_env_raw, norm_obs=True, norm_reward=True)

    if os.path.exists(GAUNTLET_SAVE_PATH):
        print(f"\n[INFO] ì§„í–‰ ì¤‘ì´ë˜ ì˜ˆì„ ì „ ëª¨ë¸({os.path.basename(GAUNTLET_SAVE_PATH)})ì„ ë¡œë“œí•˜ì—¬ ì´ì–´ê°‘ë‹ˆë‹¤.")
        model_to_train = PPO.load(GAUNTLET_SAVE_PATH, env=gauntlet_env, device="auto")
        initial_timesteps = model_to_train.num_timesteps
        print(f"[INFO] ë¡œë“œëœ ëª¨ë¸ì˜ ëˆ„ì  íƒ€ì„ìŠ¤í…: {initial_timesteps:,}")
    else:
        print(f"\n[INFO] ëª¨ë¸ {model_name}ì— ëŒ€í•œ ìƒˆ ì˜ˆì„ ì „ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        if model_to_train is None:
            print("   - ì „ë‹¬ëœ ëª¨ë¸ì´ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•˜ê³  ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            model_to_train = PPO("MlpPolicy", gauntlet_env, verbose=0, ent_coef=INITIAL_ENT_COEF_A, max_grad_norm=0.5, learning_rate=0.0001)
            initialize_to_rule_based(model_to_train)
            visualize_split_shot_debug(model_to_train)
        else:
            print(f"   - ì „ë‹¬ëœ ëª¨ë¸(Model {model_name})ì˜ í•™ìŠµ ìƒíƒœë¥¼ ìœ ì§€í•˜ë©° ì˜ˆì„ ì „ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            model_to_train.set_env(gauntlet_env)

        print("\n--- í›ˆë ¨ ì‹œì‘ ì „, ì´ˆê¸° ìƒíƒœ ì¢…í•© í‰ê°€ ì‹œì‘ ---")
        model_to_train.ent_coef = 0.0
        (win_rate, win_as_black, win_as_white, 
         reg_success, split_success, reg_ratio) = evaluate_vs_model_c(model_to_train, num_episodes_per_color=GAUNTLET_EVAL_EPISODES_PER_COLOR)
        
        with open(GAUNTLET_LOG_FILE, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([f"{model_name}_Round_0_Initial", initial_timesteps,
                             f"{win_as_black:.4f}", f"{win_as_white:.4f}", f"{win_rate:.4f}",
                             f"{reg_success:.4f}", f"{split_success:.4f}", f"{reg_ratio:.4f}"])
        print("   [INFO] ì´ˆê¸° ìƒíƒœ í‰ê°€ ê²°ê³¼ê°€ CSV ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

        visualize_vs_model_c(model_to_train, round_num=0, ppo_player_side="black")
        visualize_vs_model_c(model_to_train, round_num=0, ppo_player_side="white")

        if win_rate > 0.5:
            print(f"\nğŸ† ì´ˆê¸° ëª¨ë¸ì´ ì´ë¯¸ ì „ì²´ ìŠ¹ë¥  50%ë¥¼ ë„˜ì—ˆìŠµë‹ˆë‹¤! ì˜ˆì„ ì„ í†µê³¼í•©ë‹ˆë‹¤. ğŸ†")
            return model_to_train, model_to_train.num_timesteps
            
        print(f"   - í˜„ì¬ ëª¨ë¸ì„ ì²« ì²´í¬í¬ì¸íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤: {os.path.basename(GAUNTLET_SAVE_PATH)}")
        model_to_train.save(GAUNTLET_SAVE_PATH)
    
    original_ent_coef = model_to_train.ent_coef
    GAUNTLET_TIMESTEPS = 50000
    gauntlet_round = 1
    current_total_timesteps = model_to_train.num_timesteps

    while True:
        print(f"\n--- ì˜ˆì„  {gauntlet_round}ë¼ìš´ë“œ í›ˆë ¨ ì‹œì‘ ---")
        model_to_train.ent_coef = original_ent_coef
        
        model_to_train.learn(
            total_timesteps=GAUNTLET_TIMESTEPS,
            callback=ProgressCallback(GAUNTLET_TIMESTEPS),
            reset_num_timesteps=False
        )
        current_total_timesteps = model_to_train.num_timesteps

        print("\n--- í›ˆë ¨ í›„ ì¢…í•© í‰ê°€ ì‹œì‘ ---")
        model_to_train.ent_coef = 0.0
        (win_rate, win_as_black, win_as_white, 
         reg_success, split_success, reg_ratio) = evaluate_vs_model_c(model_to_train, num_episodes_per_color=GAUNTLET_EVAL_EPISODES_PER_COLOR)
        
        with open(GAUNTLET_LOG_FILE, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([f"{model_name}_Round_{gauntlet_round}", current_total_timesteps,
                             f"{win_as_black:.4f}", f"{win_as_white:.4f}", f"{win_rate:.4f}",
                             f"{reg_success:.4f}", f"{split_success:.4f}", f"{reg_ratio:.4f}"])
        print("   [INFO] ì˜ˆì„ ì „ ê²°ê³¼ê°€ CSV ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

        print("\n[INFO] ì˜ˆì„ ì „ ì‹œê°í™” í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (í‘ëŒ/ë°±ëŒ ê°ê° 1íšŒ).")
        visualize_vs_model_c(model_to_train, round_num=gauntlet_round, ppo_player_side="black")
        visualize_vs_model_c(model_to_train, round_num=gauntlet_round, ppo_player_side="white")

        print("\n[INFO] ë‹¤ìŒ í•™ìŠµ ì „ëµ ì„¤ì •...")
        
        # [ìˆ˜ì •] ì„±ê³µë¥ ì— ë”°ë¼ ê° ë³´ë„ˆìŠ¤ ëª¨ë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ í™œì„±í™”
        regular_bonus_active = (reg_success < 0.8)
        split_bonus_active = (split_success < 0.8)

        if regular_bonus_active:
            print(f"   âš ï¸ ì¼ë°˜ ê³µê²© ì„±ê³µë¥  ë¯¸ë‹¬({reg_success:.2%}). ë‹¤ìŒ í•™ìŠµì— 'ìì‚´ìƒ· í˜ë„í‹°'ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.")
        if split_bonus_active:
            print(f"   âš ï¸ í‹ˆìƒˆ ê³µê²© ì„±ê³µë¥  ë¯¸ë‹¬({split_success:.2%}). ë‹¤ìŒ í•™ìŠµì— 'ë…¸ì´ì¦ˆ ê°ì†Œ'ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.")
        if not regular_bonus_active and not split_bonus_active:
            print(f"   âœ… ëª¨ë“  ì„±ê³µë¥  ë‹¬ì„±. ë‹¤ìŒ í•™ìŠµì— ë³´ë„ˆìŠ¤ë¥¼ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # ë‘ í”Œë˜ê·¸ë¥¼ í™˜ê²½ì— ê°ê° ì „ë‹¬
        gauntlet_env.env_method("set_bonus_modes", 
                                regular_active=regular_bonus_active, 
                                split_active=split_bonus_active)
        
        # ë¡¤ë°± ëŒ€ì‹ , í˜„ì¬ í›ˆë ¨ëœ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ê³  ë‹¤ìŒ ë¼ìš´ë“œë¡œ ì§„í–‰
        model_to_train.save(GAUNTLET_SAVE_PATH)
        print(f"   - í˜„ì¬ ëª¨ë¸ ìƒíƒœë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {os.path.basename(GAUNTLET_SAVE_PATH)}")

        if win_rate > 0.5:
            print(f"\nğŸ† ëª¨ë¸ {model_name}ì´(ê°€) ì „ì²´ ìŠ¹ë¥  50%ë¥¼ ë„˜ì—ˆìŠµë‹ˆë‹¤! ì˜ˆì„ ì„ í†µê³¼í•©ë‹ˆë‹¤. ğŸ†")
            if os.path.exists(GAUNTLET_SAVE_PATH): os.remove(GAUNTLET_SAVE_PATH)
            break
        else:
            print(f"   - ì „ì²´ ìŠ¹ë¥ ({win_rate:.2%})ì´ 50% ë¯¸ë§Œì…ë‹ˆë‹¤. ë‹¤ìŒ ë¼ìš´ë“œ í›ˆë ¨ì„ ê³„ì†í•©ë‹ˆë‹¤.")

        gauntlet_round += 1
    
    return model_to_train, current_total_timesteps

def reload_with_env(model: PPO, new_env):
    """í˜„ì¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ë³´ì¡´í•œ ì±„, env ê°œìˆ˜ë¥¼ ë°”ê¾¸ê¸° ìœ„í•´ ì €ì¥-ì¬ë¡œë”©."""
    tmp = os.path.join(SAVE_DIR, "_tmp_reload_swap_env.zip")
    os.makedirs(SAVE_DIR, exist_ok=True)
    model.save(tmp)
    new_model = PPO.load(tmp, env=new_env, device=model.device)
    try:
        os.remove(tmp)
    except OSError:
        pass
    return new_model

def run_final_evaluation(champion_model: PPO, env):
    """
    ìµœì¢… ì±”í”¼ì–¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì—¬ëŸ¬ ì—”íŠ¸ë¡œí”¼ ë ˆë²¨ì˜ ìƒëŒ€ì™€ ë¹„êµí•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*35)
    print("ğŸ†      ìµœì¢… ì±”í”¼ì–¸ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€      ğŸ†")
    print("="*35 + "\n")
    
    opponent_ent_coefs = [0.0, 0.1, 0.2, 0.3, 0.4]
    results = []
    
    # ì±”í”¼ì–¸ ëª¨ë¸ì„ ì„ì‹œ ì €ì¥í•˜ì—¬ ê¹¨ë—í•œ ìƒíƒœì˜ ìƒëŒ€ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•¨
    champion_path = os.path.join(SAVE_DIR, "champion_model_final.zip")
    champion_model.save(champion_path)

    print(f"[*] ì±”í”¼ì–¸ ëª¨ë¸: {os.path.basename(champion_model.logger.get_dir())} ì—ì„œ ì €ì¥ë¨")
    print("[*] í‰ê°€ ìƒëŒ€: ì±”í”¼ì–¸ê³¼ ë™ì¼í•œ ëª¨ë¸ (ì—”íŠ¸ë¡œí”¼ë§Œ 0.0 ~ 0.4ë¡œ ê³ ì •)")
    print("-" * 35)

    for ent_coef in opponent_ent_coefs:
        print(f"\n[í‰ê°€] vs Opponent (ent_coef: {ent_coef:.1f})")
        
        # ì±”í”¼ì–¸ì˜ ë³µì‚¬ë³¸ì„ ìƒëŒ€ë¡œ ë¡œë“œ
        opponent_model = PPO.load(champion_path, env=env)
        opponent_model.ent_coef = ent_coef
        
        # 100íŒì”© ê³µì • í‰ê°€ ì§„í–‰
        win_rate_champion, _, _, _ = evaluate_fairly(
            champion_model, opponent_model, num_episodes=100
        )
        
        results.append((ent_coef, win_rate_champion))
        print(f"  â–¶ ì±”í”¼ì–¸ ìŠ¹ë¥ : {win_rate_champion:.2%}")

    print("\n\n" + "="*30)
    print("      ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼ ìš”ì•½ ğŸ“Š")
    print("="*30)
    print(f"{'ìƒëŒ€ ì—”íŠ¸ë¡œí”¼':<15} | {'ì±”í”¼ì–¸ ìŠ¹ë¥ ':<15}")
    print("-" * 30)
    for ent_coef, win_rate in results:
        print(f"{ent_coef:<15.1f} | {win_rate:<15.2%}")
    print("="*30)
    
    # ì„ì‹œ ì €ì¥ëœ ì±”í”¼ì–¸ ëª¨ë¸ ì‚­ì œ
    if os.path.exists(champion_path):
        os.remove(champion_path)

# --- ê²½ìŸì  í•™ìŠµ ë©”ì¸ í•¨ìˆ˜ ---
def main():
    os.makedirs(SAVE_DIR, exist_ok=True)
    os.makedirs(LOG_DIR, exist_ok=True)
    temp_env = DummyVecEnv([make_env_fn()])
    
    BEST_MODEL_FILENAME = "best_model.zip"

    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    MAIN_LOG_FILE = os.path.join(LOG_DIR, "training_log.csv")
    GAUNTLET_LOG_FILE = os.path.join(LOG_DIR, "gauntlet_log.csv")

    # ë¡œê·¸ íŒŒì¼ í—¤ë” ì´ˆê¸°í™”
    if not os.path.exists(MAIN_LOG_FILE):
        with open(MAIN_LOG_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Stage", "Total Timesteps", "Model A Entropy", "Model B Entropy", "Round 1 Win Rate (A Black)", "Round 2 Win Rate (B Black)"])
    if not os.path.exists(GAUNTLET_LOG_FILE):
        with open(GAUNTLET_LOG_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Round", "Total Timesteps", "Win Rate as Black", "Win Rate as White", "Overall Win Rate", "Neg Strategy Ratio"])
    
    # ìƒíƒœ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ì‹œì‘
    state = load_training_state() or {}
    total_timesteps_so_far = state.get("total_timesteps_so_far", 0)
    current_ent_coef_A = state.get("current_ent_coef_A", INITIAL_ENT_COEF_A)
    current_ent_coef_B = state.get("current_ent_coef_B", INITIAL_ENT_COEF_B)
    best_overall_models = state.get("best_overall_models", [])
    
    # --- [ìˆ˜ì •] ì—­ëŒ€ ìµœê³  ìŠ¹ë¥  ìƒíƒœ ë¡œë“œ ---
    best_win_rate = state.get("best_win_rate", 0.0)
    #split_shot_threshold = state.get("split_shot_threshold", 0.5)
    
    model_A, model_B = None, None
    model_pattern = re.compile(r"model_(a|b)_(\d+)_([0-9.]+)\.zip")

    # ê¸°ì¡´ ëª¨ë¸ í™•ì¸
    models_found = {"a": [], "b": []}
    if os.path.exists(SAVE_DIR):
        for f in os.listdir(SAVE_DIR):
            match = model_pattern.match(f)
            if match: 
                models_found[match.group(1)].append((int(match.group(2)), os.path.join(SAVE_DIR, f)))
    
    # ê¸°ì¡´ ëª¨ë¸ì´ ìˆëŠ” ê²½ìš° ë¡œë“œ
    if models_found["a"]:
        try:
            latest_a_path = max(models_found["a"], key=lambda i: i[0])[1]
            latest_b_path = max(models_found["b"], key=lambda i: i[0])[1] if models_found["b"] else latest_a_path
            
            print(f"[INFO] í•™ìŠµ ì´ì–´í•˜ê¸°: Model A({os.path.basename(latest_a_path)}), Model B({os.path.basename(latest_b_path)}) ë¡œë“œ")
            model_A = PPO.load(latest_a_path, env=temp_env)
            model_B = PPO.load(latest_b_path, env=temp_env)

            model_timesteps = max(model_A.num_timesteps, model_B.num_timesteps)
            if model_timesteps > total_timesteps_so_far:
                print(f"[WARN] ëª¨ë¸ì˜ íƒ€ì„ìŠ¤í…({model_timesteps:,})ì´ ìƒíƒœ íŒŒì¼({total_timesteps_so_far:,})ë³´ë‹¤ ìµœì‹ ì…ë‹ˆë‹¤. ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤.")
                total_timesteps_so_far = model_timesteps
            
            print("\n[INFO] í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì˜ ìƒíƒœë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤...")
            visualize_one_game(model_A, model_B, current_ent_coef_A, current_ent_coef_B, stage_num=0)
            
        except Exception as e:
            print(f"[WARN] ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({e}). ìƒˆ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            model_A, model_B = None, None
    else:
        print("[INFO] ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        model_A, model_B = None, None

    # ìƒˆ í•™ìŠµ ì‹œì‘ (ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)
    if model_A is None or model_B is None:

        # [âœ… ìµœì¢… ìˆ˜ì •] ì˜ˆì„ ì „ìš© VecNormalize í™˜ê²½ì„ ë¨¼ì € ìƒì„±í•©ë‹ˆë‹¤.
        N_ENVS_VS_C = 2 
        gauntlet_env_raw = make_vs_c_env_vec(n_envs=N_ENVS_VS_C)
        gauntlet_env = VecNormalize(gauntlet_env_raw, norm_obs=True, norm_reward=True)

        # [âœ… ìµœì¢… ìˆ˜ì •] ëª¨ë¸ì„ ë§Œë“¤ ë•Œë¶€í„° ì˜ˆì„ ì „ìš© í™˜ê²½(gauntlet_env)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        model_A = PPO("MlpPolicy", gauntlet_env, verbose=0, ent_coef=INITIAL_ENT_COEF_A, max_grad_norm=0.5, learning_rate=0.0001)

        print("[INFO] ëª¨ë¸ì„ Rule-based ì •ì±…ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        initialize_to_rule_based(model_A)
        print("[INFO] ì •ì±…ì„ rule-based í˜•íƒœë¡œ ê°•ì œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        total_timesteps_so_far = 0

        # ì´ˆê¸° ì˜ˆì„ ì „ì€ Aëª¨ë¸ë§Œ ì§„í–‰
        model_A, total_timesteps_so_far = run_gauntlet_training(
            model_to_train=None, 
            model_name="A", 
            initial_timesteps=0
        )
        
        print("\n[INFO] ì˜ˆì„ ì„ í†µê³¼í•œ ëª¨ë¸ Aë¥¼ ë³µì œí•˜ì—¬ ëª¨ë¸ Bë¥¼ ë‹¤ì‹œ ë™ê¸°í™”í•©ë‹ˆë‹¤...")
        post_gauntlet_a_path = os.path.join(SAVE_DIR, "model_a_post_gauntlet.zip")
        model_A.save(post_gauntlet_a_path)
        
        # ëª¨ë¸ Bë¥¼ ë¡œë“œí•  ë•Œë„ ì„ì‹œ í™˜ê²½(temp_env)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        model_B = PPO.load(post_gauntlet_a_path, env=temp_env)
        print("[INFO] ëª¨ë¸ B ë™ê¸°í™” ì™„ë£Œ.")
        '''''''''''
        try:
            params = model_A.get_parameters()
            params['policy']['action_net.weight'].data.fill_(0)
            params['policy']['action_net.bias'].data.fill_(0)
            model_A.set_parameters(params)
            print("[INFO] ì¶”ê°€ ì´ˆê¸°í™”(action_net->0) ì„±ê³µ.")
        except KeyError:
            print("[ê²½ê³ ] ëª¨ë¸ êµ¬ì¡°ë¥¼ ì°¾ì§€ ëª»í•´ ì¶”ê°€ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print_model_parameters(model_A)
        '''''''''''

    # --- ë©”ì¸ í•™ìŠµ ë£¨í”„ ---
    start_stage = total_timesteps_so_far // TIMESTEPS_PER_STAGE if TIMESTEPS_PER_STAGE > 0 else 0
    total_expected_timesteps = MAX_STAGES * TIMESTEPS_PER_STAGE

    VEC_NORMALIZE_STATS_PATH = os.path.join(SAVE_DIR, "vec_normalize.pkl")

    # VecNormalize í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    # ìƒëŒ€ ëª¨ë¸ì€ ë£¨í”„ ì•ˆì—ì„œ ê³„ì† ë°”ë€Œë¯€ë¡œ, ì„ì‹œ ìƒëŒ€ë¡œ ë¨¼ì € ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    temp_opponent_model = model_B if model_B is not None else model_A
    train_env_raw = make_vs_opponent_env_vec(opponent_model=temp_opponent_model, n_envs=2)
    train_env = VecNormalize(train_env_raw, norm_obs=True, norm_reward=True)

    # ì €ì¥ëœ ì •ê·œí™” ìƒíƒœê°€ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    if os.path.exists(VEC_NORMALIZE_STATS_PATH):
        print(f"[INFO] VecNormalize ìƒíƒœ ë¡œë“œ: {VEC_NORMALIZE_STATS_PATH}")
        train_env = VecNormalize.load(VEC_NORMALIZE_STATS_PATH, train_env_raw)
        # VecEnvëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í™˜ê²½ì„ ë‹¤ì‹œ ë§Œë“¤ë¯€ë¡œ, ë˜í•‘ì„ ë‹¤ì‹œ í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.
        train_env.norm_obs = True
        train_env.norm_reward = True

    for stage_idx in range(start_stage, MAX_STAGES):
        if total_timesteps_so_far >= total_expected_timesteps: break
        print_overall_progress(stage_idx + 1, MAX_STAGES, total_timesteps_so_far, total_expected_timesteps)
        print(f"\n--- ìŠ¤í…Œì´ì§€ {stage_idx + 1}/{MAX_STAGES} ì‹œì‘ ---")
        stage_start_time = time.time()
        
        current_training_model_name = "A" if current_ent_coef_A >= current_ent_coef_B else "B"
        model_to_train, ent_coef_train = (model_A, current_ent_coef_A) if current_training_model_name == "A" else (model_B, current_ent_coef_B)
        
        model_to_train.ent_coef = ent_coef_train
        opponent_model = model_B if current_training_model_name == "A" else model_A

        # [âœ… ìµœì¢… ìˆ˜ì •] ì•„ë˜ 3ì¤„ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì‚­ì œí•©ë‹ˆë‹¤.
        train_env.env_method("set_opponent", opponent_model)
        model_to_train.set_env(train_env)
        print(f"   í•™ìŠµ ëŒ€ìƒ: Model {current_training_model_name} (ent_coef: {ent_coef_train:.5f})")
        
        model_to_train.learn(total_timesteps=TIMESTEPS_PER_STAGE, callback=ProgressCallback(TIMESTEPS_PER_STAGE), reset_num_timesteps=False)
        total_timesteps_so_far = model_to_train.num_timesteps
        if current_training_model_name == "A": model_A = model_to_train
        else: model_B = model_to_train

        print(f"\n   --- ê²½ìŸ í‰ê°€ ì‹œì‘ ---")
        win_rate_A, win_rate_B, r1_win_rate, r2_win_rate = evaluate_fairly(model_A, model_B, num_episodes=EVAL_EPISODES_FOR_COMPETITION)
        
        with open(MAIN_LOG_FILE, 'a', newline='') as f:
            writer = csv.writer(f)
            log_data = [stage_idx + 1, total_timesteps_so_far, f"{current_ent_coef_A:.5f}", f"{current_ent_coef_B:.5f}", f"{r1_win_rate:.4f}", f"{r2_win_rate:.4f}"]
            writer.writerow(log_data)
        print("   [INFO] í•™ìŠµ ê²°ê³¼ê°€ CSV ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

        visualize_one_game(model_A, model_B, current_ent_coef_A, current_ent_coef_B, stage_idx + 1, force_A_as_black=(current_training_model_name == 'A'))
        visualize_one_game(model_A, model_B, current_ent_coef_A, current_ent_coef_B, stage_idx + 1, force_A_as_black=(current_training_model_name != 'A'))

        print("\n   --- ì—”íŠ¸ë¡œí”¼ ë° ìµœê³  ëª¨ë¸ ê²°ì • ---")
        if win_rate_A > win_rate_B: effective_winner, winner_win_rate = "A", win_rate_A
        elif win_rate_B > win_rate_A: effective_winner, winner_win_rate = "B", win_rate_B
        else:
            effective_winner = "B" if current_training_model_name == "A" else "A"
            winner_win_rate = win_rate_B if effective_winner == "B" else win_rate_A
        
        if win_rate_A == win_rate_B: print(f"   ê²½ìŸ ê²°ê³¼: ë¬´ìŠ¹ë¶€. í•™ìŠµ ëŒ€ìƒ({current_training_model_name})ì´ íŒ¨ë°°í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ Model {effective_winner} ìŠ¹ë¦¬.")
        else: print(f"   ê²½ìŸ ê²°ê³¼: Model {effective_winner} ìŠ¹ë¦¬ (ìŠ¹ë¥ : {winner_win_rate:.2%})")

        # --- [ìˆ˜ì •] ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ë¡œì§ ---
        champion_model = model_A if effective_winner == "A" else model_B
        BEST_MODEL_PATH = os.path.join(SAVE_DIR, BEST_MODEL_FILENAME)
        if winner_win_rate > best_win_rate:
            print(f"   ğŸš€ ìƒˆë¡œìš´ ìµœê³  ìŠ¹ë¥  ë‹¬ì„±! (ì´ì „: {best_win_rate:.2%} -> í˜„ì¬: {winner_win_rate:.2%})")
            print(f"   '{BEST_MODEL_FILENAME}' íŒŒì¼ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
            champion_model.save(BEST_MODEL_PATH)
            best_win_rate = winner_win_rate  # ìµœê³  ìŠ¹ë¥  ê°±ì‹ 
        else:
            print(f"   ìµœê³  ìŠ¹ë¥ ({best_win_rate:.2%})ì„ ë„˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í˜„ì¬: {winner_win_rate:.2%})")

        FINAL_EVAL_ENT_THRESHOLD = 0.45
        should_terminate = False
        model_to_requalify, model_to_requalify_name = None, ""

        if effective_winner == "A" and current_ent_coef_A > current_ent_coef_B:
            new_ent_coef_B = min(current_ent_coef_B + ENT_COEF_INCREMENT, MAX_ENT_COEF)
            if new_ent_coef_B != current_ent_coef_B:
                print(f"   Model B ì—”íŠ¸ë¡œí”¼ ì¦ê°€ â†’ {new_ent_coef_B:.5f}")
                current_ent_coef_B, model_to_requalify, model_to_requalify_name = new_ent_coef_B, model_B, "B"
            if new_ent_coef_B >= FINAL_EVAL_ENT_THRESHOLD:
                run_final_evaluation(champion_model=model_A, env=temp_env); should_terminate = True

        elif effective_winner == "B" and current_ent_coef_B > current_ent_coef_A:
            new_ent_coef_A = min(current_ent_coef_A + ENT_COEF_INCREMENT, MAX_ENT_COEF)
            if new_ent_coef_A != current_ent_coef_A:
                print(f"   Model A ì—”íŠ¸ë¡œí”¼ ì¦ê°€ â†’ {new_ent_coef_A:.5f}")
                current_ent_coef_A, model_to_requalify, model_to_requalify_name = new_ent_coef_A, model_A, "A"
            if new_ent_coef_A >= FINAL_EVAL_ENT_THRESHOLD:
                run_final_evaluation(champion_model=model_B, env=temp_env); should_terminate = True
        else:
            print("   ì—”íŠ¸ë¡œí”¼ ì¡°ì • ì—†ìŒ")

        if model_to_requalify:
            trained_model, total_timesteps_so_far = run_gauntlet_training(
                model_to_train=model_to_requalify, 
                model_name=model_to_requalify_name, 
                initial_timesteps=total_timesteps_so_far
            )
            if model_to_requalify_name == "A": model_A = trained_model
            else: model_B = trained_model

        model_A_path = os.path.join(SAVE_DIR, f"model_a_{total_timesteps_so_far}_{current_ent_coef_A:.3f}.zip")
        model_A.save(model_A_path)
        best_overall_models = update_best_models(best_overall_models, model_A_path, win_rate_A)
        
        model_B_path = os.path.join(SAVE_DIR, f"model_b_{total_timesteps_so_far}_{current_ent_coef_B:.3f}.zip")
        model_B.save(model_B_path)

        # [âœ… ì¶”ê°€] VecNormalize ìƒíƒœ ì €ì¥
        train_env.save(VEC_NORMALIZE_STATS_PATH)
        print(f" ğŸ’¾  VecNormalize ìƒíƒœë¥¼ {os.path.basename(VEC_NORMALIZE_STATS_PATH)} íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        clean_models(model_A_path, model_B_path, [m[0] for m in best_overall_models])
        
        # --- [ìˆ˜ì •] ìƒíƒœ ì €ì¥ ì‹œ ìµœê³  ìŠ¹ë¥  í¬í•¨ ---
        current_state = {
            "total_timesteps_so_far": total_timesteps_so_far, "current_ent_coef_A": current_ent_coef_A,
            "current_ent_coef_B": current_ent_coef_B, "best_overall_models": best_overall_models,
            "best_win_rate": best_win_rate,
            #"split_shot_threshold": split_shot_threshold 
        }
        save_training_state(current_state)
        
        minutes, seconds = divmod(int(time.time() - stage_start_time), 60)
        print(f"\n[ìŠ¤í…Œì´ì§€ {stage_idx + 1}] ì™„ë£Œ (ì†Œìš” ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ)")

        if should_terminate:
            print("\n--- ìµœì¢… í‰ê°€ ì™„ë£Œ. í•™ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ---")
            break

    print("\n--- ì „ì²´ ê²½ìŸì  í•™ìŠµ ì™„ë£Œ ---")
    temp_env.close()


# --- ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œ í†µí•© í•¨ìˆ˜ë“¤ ---
def check_and_run_specialized_training(current_model_path=None, gauntlet_log_path="rl_logs_competitive/gauntlet_log.csv"):
    """
    ì„±ëŠ¥ ë¶„ì„ í›„ í•„ìš”ì‹œ ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œ ì‹¤í–‰
    """
    if not SPECIALIZED_TRAINING_AVAILABLE:
        print("[Warning] ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print("\n=== ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œ ê²€ì‚¬ ===")
    
    # ì „ìš© í›ˆë ¨ì†Œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = SpecializedTrainingManager(current_model_path)
    
    # ì„±ëŠ¥ ë¶„ì„
    regular_success_rate, split_success_rate = manager.analyze_performance(gauntlet_log_path)
    
    if regular_success_rate is None or split_success_rate is None:
        print("[Warning] ì„±ëŠ¥ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None
    
    # í›ˆë ¨ í•„ìš”ì„± í™•ì¸
    needs_regular = manager.needs_regular_training(regular_success_rate)
    needs_split = manager.needs_split_training(split_success_rate)
    
    if not needs_regular and not needs_split:
        print("âœ… ëª¨ë“  ì„±ê³µë¥ ì´ ì¶©ë¶„í•©ë‹ˆë‹¤. ì „ìš© í›ˆë ¨ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    # ì „ìš© í›ˆë ¨ ì‹¤í–‰
    print("ğŸ”§ ì „ìš© í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤. ì „ìš© í›ˆë ¨ì†Œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    trained_models = manager.run_specialized_training_cycle(gauntlet_log_path)
    
    if trained_models:
        print("âœ… ì „ìš© í›ˆë ¨ ì™„ë£Œ!")
        manager.visualize_training_results()
        return trained_models
    else:
        print("âŒ ì „ìš© í›ˆë ¨ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None


def integrate_specialized_training_into_main_loop():
    """
    ë©”ì¸ í›ˆë ¨ ë£¨í”„ì— ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œ í†µí•©
    """
    if not SPECIALIZED_TRAINING_AVAILABLE:
        return
    
    print("\n=== ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œ í†µí•© ===")
    print("ë©”ì¸ í›ˆë ¨ ë£¨í”„ì— ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ì„±ê³µë¥ ì´ ë¯¸ë‹¬ë  ë•Œ ìë™ìœ¼ë¡œ ì „ìš© í›ˆë ¨ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")


if __name__ == "__main__":
    # ì „ìš© í›ˆë ¨ì†Œ ì‹œìŠ¤í…œ í†µí•© í™•ì¸
    integrate_specialized_training_into_main_loop()
    
    # ë©”ì¸ í›ˆë ¨ ì‹¤í–‰
    main()
    
    # í›ˆë ¨ ì™„ë£Œ í›„ ì „ìš© í›ˆë ¨ì†Œ ê²€ì‚¬
    print("\n=== í›ˆë ¨ ì™„ë£Œ í›„ ì „ìš© í›ˆë ¨ì†Œ ê²€ì‚¬ ===")
    check_and_run_specialized_training()