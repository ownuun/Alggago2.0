import numpy as np
import os
from stable_baselines3 import PPO
from pymunk import Vec2d
from physics import scale_force
import re
from opponent import get_regular_action, get_split_shot_action

# --- [ì¶”ê°€] íŒŒì¼ ê¸°ì¤€ ê²½ë¡œ ìœ í‹¸ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
def rel_path(*parts):
    return os.path.join(BASE_DIR, *parts)

MODEL_SAVE_DIR = rel_path("rl_models_competitive")

class MainRLAgent:
    """
    ë©”ì¸ ê°•í™” í•™ìŠµ ëª¨ë¸ì˜ ë¡œë”© ë° ì¶”ë¡  ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self, model_path=None):
        self.model = None
        # --- [ìˆ˜ì •] ëª¨ë¸ ê²½ë¡œë¥¼ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥ ---
        self.model_path = model_path
        if model_path and os.path.exists(model_path):
            try:
                self.model = PPO.load(model_path)
                print(f"[MainRLAgent] ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
            except Exception as e:
                print(f"[MainRLAgent] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_path} - {e}")
                self.model = None
        elif model_path:
            print(f"[MainRLAgent] ì§€ì •ëœ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")

    def select_action(self, observation: np.ndarray):
        """
        ì£¼ì–´ì§„ ê´€ì¸¡ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ í–‰ë™ ë²¡í„°(5ê°œ ê°’)ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        AlggaGo1.0 ëª¨ë¸ì˜ ê²½ìš° í˜¸í™˜ì„±ì„ ë³´ì •í•©ë‹ˆë‹¤.
        """
        if self.model is None:
            print(f"[MainRLAgent] ëª¨ë¸ ì—†ìŒ: ìˆœìˆ˜ rule-based í–‰ë™")
            return np.array([1.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)

        observation_reshaped = np.expand_dims(observation, axis=0)
        action_raw, _states = self.model.predict(observation_reshaped, deterministic=True)
        action_vector = action_raw[0]
        
        # --- [âœ… í•µì‹¬ ìˆ˜ì •] AlggaGo1.0 ëª¨ë¸ í˜¸í™˜ì„± ì²˜ë¦¬ ---
        if self.model_path and "AlggaGo1.0" in self.model_path:
            # 1.0 ëª¨ë¸ì´ 3ê°œì˜ ê°’ì„ ë°˜í™˜í•˜ëŠ” ê²½ìš°, 5ê°œ ê°’ìœ¼ë¡œ ë³€í™˜
            if action_vector.size == 3:
                print("[MainRLAgent] AlggaGo1.0 í˜¸í™˜ ëª¨ë“œ: 'ì¼ë°˜ ê³µê²©'ì„ ê°•ì œí•˜ê³  3-val ì¶œë ¥ì„ 5-valë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
                # ì „ëµ ê°’: [ì¼ë°˜ê³µê²© ì„ í˜¸ë„, í‹ˆìƒˆê³µê²© ì„ í˜¸ë„] -> [1.0, -1.0]ìœ¼ë¡œ ì¼ë°˜ ê³µê²© ê°•ì œ
                pref_regular = 1.0
                pref_split = -1.0
                # ëª¨ë¸ì´ ë°˜í™˜í•œ 3ê°œì˜ íŒŒë¼ë¯¸í„° ê°’
                raw_index, raw_angle, raw_force = action_vector
                # 5ê°œ ê°’ì˜ ìƒˆë¡œìš´ í–‰ë™ ë²¡í„° ìƒì„±
                return np.array([pref_regular, pref_split, raw_index, raw_angle, raw_force], dtype=np.float32)

        # ê·¸ ì™¸ ìµœì‹  ëª¨ë¸ë“¤ì€ ê·¸ëŒ€ë¡œ 5ê°œ ê°’ ë°˜í™˜
        return action_vector

def apply_action_to_stone(full_action: np.ndarray, stones: list, target_color_tuple: tuple):
    """
    ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì „ì²´ í–‰ë™ ë²¡í„°ë¥¼ í•´ì„í•˜ì—¬ ëŒì— impulseë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    """
    if full_action is None:
        return

    player_stones = [s for s in stones if s.color[:3] == target_color_tuple]
    opponent_stones = [s for s in stones if s.color[:3] != target_color_tuple]

    if not player_stones:
        print(f"[ERROR] apply_action_to_stone: ëŒ€ìƒ ëŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # --- [âœ… í•µì‹¬ ìˆ˜ì •] argmaxë¥¼ softmax ìƒ˜í”Œë§ ë¡œì§ìœ¼ë¡œ êµì²´ ---
    strategy_preferences = np.asarray(full_action[:2], dtype=np.float32)
    
    # ì•ˆì •í™”ëœ ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥  ê³„ì‚° (train.pyì™€ ë™ì¼í•œ ë¡œì§)
    max_pref = float(np.max(strategy_preferences))
    exp_p = np.exp(strategy_preferences - max_pref)
    probs = exp_p / (np.sum(exp_p) + 1e-8)

    # ìˆ˜ì¹˜ ì´ìƒ ì‹œ ì•ˆì „í•˜ê²Œ argmaxë¡œ í´ë°±
    if not np.all(np.isfinite(probs)) or probs.sum() <= 0:
        strategy_choice = int(np.argmax(strategy_preferences))
    else:
        # ê³„ì‚°ëœ í™•ë¥ ì— ë”°ë¼ í–‰ë™ì„ ëœë¤í•˜ê²Œ ì„ íƒ
        strategy_choice = int(np.random.choice(2, p=probs))
    # --- ìˆ˜ì • ë ---

    if strategy_choice == 1: # 1ë²ˆ ì „ëµ: í‹ˆìƒˆ ê³µê²©
        rule_action = get_split_shot_action(player_stones, opponent_stones)
        if rule_action is None:
            rule_action = get_regular_action(player_stones, opponent_stones)
            print("[Action Strategy] ëª¨ë¸ì´ 'í‹ˆìƒˆ ê³µê²©'ì„ ì›í–ˆìœ¼ë‚˜, ë¶ˆê°€ëŠ¥í•˜ì—¬ 'ì¼ë°˜ ê³µê²©'ìœ¼ë¡œ ì „í™˜")
        else:
            print("[Action Strategy] ëª¨ë¸ì´ 'í‹ˆìƒˆ ê³µê²©(Split Shot)'ì„ ì„ íƒ")
    else: # 0ë²ˆ ì „ëµ: ì¼ë°˜ ê³µê²©
        rule_action = get_regular_action(player_stones, opponent_stones)
        print("[Action Strategy] ëª¨ë¸ì´ 'ì¼ë°˜ ê³µê²©(Regular Action)'ì„ ì„ íƒ")

    if rule_action is None:
        print(f"[ERROR] apply_action_to_stone: Rule-based í–‰ë™ ê³„ì‚° ì‹¤íŒ¨")
        return
        
    raw_index, raw_angle, raw_force = full_action[2:]
    
    index_weight = np.clip(raw_index, -1.0, 1.0)
    angle_offset = np.clip(raw_angle, -1.0, 1.0)
    force_offset = np.clip(raw_force, -1.0, 1.0)
    
    exploration_range = {"index": 1.0, "angle": np.pi / 4, "force": 0.5}

    final_index_weight = index_weight * exploration_range['index']
    final_angle_offset = angle_offset * exploration_range['angle']
    
    final_index_weight = index_weight * exploration_range['index']
    final_angle_offset = angle_offset * exploration_range['angle']
    final_force_offset = force_offset * exploration_range['force']

    print(f"[MainRLAgent] AI ì˜¤ì°¨ ì˜ˆì¸¡ - index_w: {final_index_weight:.3f}, angle_o: {final_angle_offset:.3f}, force_o: {final_force_offset:.3f}")

    rule_idx, rule_angle, rule_force = rule_action

    if len(player_stones) > 1:
        # 2. train.pyì™€ ë™ì¼í•˜ê²Œ scaling ë¶€ë¶„ ì‚­ì œ
        idx_offset = int(np.round(final_index_weight))
        final_idx = np.clip(rule_idx + idx_offset, 0, len(player_stones) - 1)
    else:
        final_idx = 0

    final_angle = rule_angle + final_angle_offset
    final_force = np.clip(rule_force + final_force_offset, 0.0, 1.0)

    print(f"[apply_action] Rule: idx={rule_idx}, angle={rule_angle:.2f}, force={rule_force:.2f}")
    print(f"[apply_action] Final: idx={final_idx}, angle={final_angle:.2f}, force={final_force:.2f}")

    stone_to_move = player_stones[final_idx]
    direction = Vec2d(1, 0).rotated(final_angle)
    scaled_force = scale_force(final_force)
    impulse = direction * scaled_force
    stone_to_move.body.apply_impulse_at_world_point(impulse, stone_to_move.body.position)

def choose_ai():
    """
    ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ë©”ì¸ ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
    (ì´ í•¨ìˆ˜ëŠ” ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤)
    """
    print("ğŸ¤– AI í–‰ë™ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    
    model_pattern = re.compile(r"model_(a|b)_(\d+)_([0-9.]+)\.zip")
    available_models = []

    if os.path.exists(MODEL_SAVE_DIR):
        for filename in sorted(os.listdir(MODEL_SAVE_DIR)):
            match = model_pattern.match(filename)
            if match:
                model_char = match.group(1).upper()
                timesteps = int(match.group(2))
                entropy = float(match.group(3))
                full_path = os.path.join(MODEL_SAVE_DIR, filename)
                available_models.append({
                    "char": model_char, "timesteps": timesteps,
                    "entropy": entropy, "path": full_path
                })
    
    available_models.sort(key=lambda x: x["timesteps"])

    print("   0) ìˆœìˆ˜ Rule-based (AI ì—†ìŒ)")
    model_options = {"0": None}
    
    if not available_models:
        print("   (ì‚¬ìš© ê°€ëŠ¥í•œ ê°•í™”í•™ìŠµ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.)")
    else:
        for i, model_info in enumerate(available_models):
            option_num = i + 1
            print(f"   {option_num}) ê°•í™”í•™ìŠµ ëª¨ë¸ (Model {model_info['char']} - {model_info['timesteps']} steps, ent={model_info['entropy']:.3f})")
            model_options[str(option_num)] = model_info["path"]

    choice = input("ì„ íƒ (0" + "".join(f"/{i+1}" for i in range(len(available_models))) + "): ").strip()
    model_path = model_options.get(choice)
    
    if choice in model_options:
        if model_path:
            print(f"[AI ì„ íƒ] ê°•í™”í•™ìŠµ ëª¨ë¸ '{os.path.basename(model_path)}'ì„(ë¥¼) ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            print("[AI ì„ íƒ] ìˆœìˆ˜ Rule-basedë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        print("[AI ì„ íƒ] ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤. ìˆœìˆ˜ Rule-basedë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        model_path = None

    return MainRLAgent(model_path=model_path)