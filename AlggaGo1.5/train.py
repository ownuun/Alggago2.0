import os
import time
import re
import numpy as np
import torch
import pygame
import csv 
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from tqdm import tqdm
from opponent_c import model_c_action
from opponent import get_regular_action, get_split_shot_action 

import gymnasium as gym
from gymnasium import spaces

from env import AlggaGoEnv
from physics import WIDTH, HEIGHT, all_stones_stopped, MARGIN

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì • ---
MAX_STAGES = 300
TIMESTEPS_PER_STAGE = 50000
SAVE_DIR = "rl_models_competitive"
LOG_DIR = "rl_logs_competitive"
INITIAL_ENT_COEF_A = 0.15
INITIAL_ENT_COEF_B = 0.1
ENT_COEF_INCREMENT = 0.1
MAX_ENT_COEF = 0.5
EVAL_EPISODES_FOR_COMPETITION = 200

# --- ì§„í–‰ë¥  í‘œì‹œ ì½œë°± í´ë˜ìŠ¤ ---
class ProgressCallback(BaseCallback):
    def __init__(self, total_timesteps):
        super().__init__()
        self.total_timesteps = total_timesteps
        self.pbar = None

    def _on_training_start(self):
        # ì‹¤ì œ ëª©í‘œ íƒ€ì„ìŠ¤í…(= learnì— ë„˜ê¸´ total_timesteps)ë¡œ í‘œì‹œ
        self.start_num = self.model.num_timesteps
        self.pbar = tqdm(total=self.total_timesteps,
                         desc="í•™ìŠµ ì§„í–‰ë¥ ",
                         bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')

    def _on_step(self):
        # í˜„ì¬ ì§„í–‰ íƒ€ì„ìŠ¤í… = (ëª¨ë¸ ëˆ„ì ) - (í•™ìŠµ ì‹œì‘ ì‹œì )
        done_ts = self.model.num_timesteps - self.start_num
        # pbar ìœ„ì¹˜ë¥¼ ì§ì ‘ ë§ì¶°ì¤Œ
        if self.pbar:
            self.pbar.n = min(done_ts, self.total_timesteps)
            self.pbar.refresh()
        return True

    def _on_training_end(self):
        if self.pbar: self.pbar.close(); self.pbar = None
# --- Rule-based ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_to_rule_based(model, angle_value=-1.57, force_value=1.0):
    with torch.no_grad():
        policy_net = model.policy.mlp_extractor.policy_net
        action_net = model.policy.action_net
        for layer in policy_net:
            if hasattr(layer, 'weight'): layer.weight.fill_(0.0)
            if hasattr(layer, 'bias'): layer.bias.fill_(0.0)
        for i in range(action_net.out_features):
            action_net.weight[i].fill_(0.0)
            action_net.bias[i].fill_(angle_value if i == 0 else force_value)
        if isinstance(model.policy.log_std, torch.nn.Parameter):
            model.policy.log_std.data.fill_(-20.0)

# --- ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸ í•¨ìˆ˜ ---
def print_model_parameters(model: PPO):
    print("\n==== ëª¨ë¸ ì´ˆê¸° íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸ ====")
    params = model.get_parameters()['policy']
    for name, tensor in params.items():
        if name in ["mlp_extractor", "value_net"] and isinstance(tensor, dict):
            for sub_name, sub_tensor in tensor.items():
                if hasattr(sub_tensor, 'cpu'):
                    arr = sub_tensor.cpu().numpy()
                    print(f"policy.{name}.{sub_name}: max={np.max(arr):.6f}, min={np.min(arr):.6f}")
        elif hasattr(tensor, 'cpu'):
            arr = tensor.cpu().numpy()
            print(f"policy.{name}: max={np.max(arr):.6f}, min={np.min(arr):.6f}")
    print("=" * 31, "\n")

# --- í™˜ê²½ ìƒì„± í—¬í¼ í•¨ìˆ˜ ---
def make_env_fn():
    def _init():
        env = AlggaGoEnv()
        monitored_env = Monitor(env, filename=LOG_DIR)
        return monitored_env
    return _init

# --- ê¸°íƒ€ í—¬í¼ í•¨ìˆ˜ ---
def clean_models(model_A_path, model_B_path, best_model_paths):
    if not os.path.exists(SAVE_DIR): return
    all_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(".zip")]
    to_keep_names = {os.path.basename(p) for p in [model_A_path, model_B_path] if p} | {os.path.basename(p) for p in best_model_paths}
    for fname in all_files:
        if fname in to_keep_names: continue
        try:
            file_to_remove = os.path.join(SAVE_DIR, fname)
            if os.path.exists(file_to_remove): os.remove(file_to_remove)
        except OSError as e: print(f"[WARN] íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
def update_best_models(current_best_models, new_model_path, reward, max_to_keep=5):
    current_best_models.append((new_model_path, reward))
    current_best_models.sort(key=lambda x: x[1], reverse=True)
    return current_best_models[:max_to_keep]
TRAINING_STATE_FILE = os.path.join(SAVE_DIR, "training_state.npy")
def load_training_state():
    if os.path.exists(TRAINING_STATE_FILE):
        try:
            state = np.load(TRAINING_STATE_FILE, allow_pickle=True).item()
            print(f"[INFO] ì´ì „ í•™ìŠµ ìƒíƒœ ë¡œë“œ ì„±ê³µ: {state}")
            return state
        except Exception as e: print(f"[ERROR] í•™ìŠµ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None
def save_training_state(state_dict):
    os.makedirs(SAVE_DIR, exist_ok=True)
    np.save(TRAINING_STATE_FILE, state_dict)
def print_overall_progress(current_stage, total_stages, current_timesteps, total_timesteps):
    stage_progress = (current_stage / total_stages) * 100
    timestep_progress = (current_timesteps / total_timesteps) * 100
    print(f"\n{'='*60}\nğŸ“Š ì „ì²´ í•™ìŠµ ì§„í–‰ë¥ \n   ìŠ¤í…Œì´ì§€: {current_stage}/{total_stages} ({stage_progress:.1f}%)\n   íƒ€ì„ìŠ¤í…: {current_timesteps:,}/{total_timesteps:,} ({timestep_progress:.1f}%)\n{'='*60}")

# --- ê³µì • í‰ê°€(Fair Evaluation) í•¨ìˆ˜ ---
def evaluate_fairly(model_A: PPO, model_B: PPO, num_episodes: int):
    games_per_round = num_episodes // 2
    if games_per_round == 0: return 0.5, 0.5, 0.0, 0.0
    print(f"   - ê³µì •í•œ í‰ê°€: ì´ {num_episodes} ê²Œì„ ({games_per_round} ê²Œì„/ë¼ìš´ë“œ)")

    def _play_round(black_model: PPO, white_model: PPO, num_games: int, round_name: str):
        black_wins = 0
        env = Monitor(AlggaGoEnv())
        for _ in tqdm(range(num_games), desc=round_name, leave=False):
            obs, _ = env.reset(options={"initial_player": "black"})
            done = False
            while not done:
                current_player = env.env.current_player
                action_model = black_model if current_player == 'black' else white_model
                action, _ = action_model.predict(obs, deterministic=True)
                squeezed_action = np.squeeze(action)
                obs, _, done, _, info = env.step(squeezed_action)
            if info.get('winner') == 'black': black_wins += 1
        env.close()
        return black_wins / num_games if num_games > 0 else 0

    # í‰ê°€ ì „ ì›ë˜ ì—”íŠ¸ë¡œí”¼ ì €ì¥ ë° 0ìœ¼ë¡œ ê³ ì •
    original_ent_A = model_A.ent_coef
    original_ent_B = model_B.ent_coef
    win_rate_A, win_rate_B, r1_black_win_rate, r2_black_win_rate = (0.5, 0.5, 0, 0)
    try:
        print("   [INFO] í‰ê°€ë¥¼ ìœ„í•´ ë‘ ëª¨ë¸ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ 0.0ìœ¼ë¡œ ê³ ì •í•©ë‹ˆë‹¤.")
        model_A.ent_coef = 0.0
        model_B.ent_coef = 0.0

        r1_black_win_rate = _play_round(model_A, model_B, games_per_round, "1ë¼ìš´ë“œ (Aê°€ í‘ëŒ)")
        print(f"   â–¶ 1ë¼ìš´ë“œ (Model A í‘ëŒ) ìŠ¹ë¥ : {r1_black_win_rate:.2%}")
        r2_black_win_rate = _play_round(model_B, model_A, games_per_round, "2ë¼ìš´ë“œ (Bê°€ í‘ëŒ)")
        print(f"   â–¶ 2ë¼ìš´ë“œ (Model B í‘ëŒ) ìŠ¹ë¥ : {r2_black_win_rate:.2%}")
        win_rate_A = (r1_black_win_rate + (1 - r2_black_win_rate)) / 2
        win_rate_B = (r2_black_win_rate + (1 - r1_black_win_rate)) / 2
    finally:
        # í‰ê°€ í›„ ì›ë˜ ì—”íŠ¸ë¡œí”¼ë¡œ ë³µì›
        model_A.ent_coef = original_ent_A
        model_B.ent_coef = original_ent_B
        print("   [INFO] ì›ë˜ ì—”íŠ¸ë¡œí”¼ ê°’ìœ¼ë¡œ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return win_rate_A, win_rate_B, r1_black_win_rate, r2_black_win_rate

def evaluate_vs_model_c(ppo_model: PPO, num_episodes_per_color: int):
    """PPO ëª¨ë¸ê³¼ ëª¨ë¸ Cì˜ ìŠ¹ë¥ ì„ í‘/ë°± ê°ê° í‰ê°€í•˜ê³  ìƒì„¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    print(f"   - ëª¨ë¸ Cì™€ íŠ¹ë³„ í‰ê°€: ì´ {num_episodes_per_color * 2} ê²Œì„ (í‘/ë°± ê° {num_episodes_per_color}íŒ)")
    env = AlggaGoEnv()
    from pymunk import Vec2d
    from physics import scale_force, all_stones_stopped

    win_rates = {}
    total_wins = 0

    ppo_turn_count = 0
    neg_strategy_count = 0
    
    for side in ["black", "white"]:
        ppo_wins_on_side = 0
        desc = f"   PPO({side}) vs C"
        
        for _ in tqdm(range(num_episodes_per_color), desc=desc, leave=False):
            obs, _ = env.reset(options={"initial_player": side})
            done = False
            info = {}
            while not done:
                current_player_color = env.current_player
                
                if current_player_color == side:  # PPO ëª¨ë¸ì˜ í„´
                    action, _ = ppo_model.predict(obs, deterministic=True)
                    strategy_choice = np.squeeze(action)[0]
                    if strategy_choice < 0:
                        neg_strategy_count += 1
                    ppo_turn_count += 1
                    obs, _, done, _, info = env.step(action)
                else:  # ëª¨ë¸ Cì˜ í„´ (ì§ì ‘ ì‹¤í–‰)
                    absolute_action = model_c_action(env.stones, current_player_color)
                    if absolute_action:
                        idx, angle, force = absolute_action
                        player_stones = [s for s in env.stones if s.color[:3] == ((0,0,0) if current_player_color=="black" else (255,255,255))]
                        if 0 <= idx < len(player_stones):
                            stone_to_shoot = player_stones[idx]
                            direction = Vec2d(1, 0).rotated(angle)
                            impulse = direction * scale_force(force)
                            stone_to_shoot.body.apply_impulse_at_world_point(impulse, stone_to_shoot.body.position)

                    physics_steps = 0
                    while not all_stones_stopped(env.stones) and physics_steps < 600:
                        env.space.step(1/60.0)
                        physics_steps += 1

                    for shape in env.stones[:]:
                        if not (MARGIN < shape.body.position.x < WIDTH - MARGIN and MARGIN < shape.body.position.y < HEIGHT - MARGIN):
                            env.space.remove(shape, shape.body)
                            env.stones.remove(shape)
                    
                    current_black = sum(1 for s in env.stones if s.color[:3] == (0,0,0))
                    current_white = sum(1 for s in env.stones if s.color[:3] == (255,255,255))
                    
                    if current_black == 0:
                        done = True; info['winner'] = 'white'
                    elif current_white == 0:
                        done = True; info['winner'] = 'black'

                    env.current_player = "white" if env.current_player == "black" else "black"
                    obs = env._get_obs()

            if done and info.get('winner') == side:
                ppo_wins_on_side += 1
        
        win_rate = ppo_wins_on_side / num_episodes_per_color if num_episodes_per_color > 0 else 0
        print(f"   â–¶ PPOê°€ {side}ì¼ ë•Œ ìŠ¹ë¥ : {win_rate:.2%}")
        win_rates[side] = win_rate
        total_wins += ppo_wins_on_side

    env.close()
    overall_win_rate = total_wins / (num_episodes_per_color * 2) if num_episodes_per_color > 0 else 0
    neg_strategy_ratio = neg_strategy_count / ppo_turn_count if ppo_turn_count > 0 else 0
    print(f"   â–¶ ëª¨ë¸ PPO ì „ì²´ ìŠ¹ë¥  (vs C): {overall_win_rate:.2%}")
    print(f"   â–¶ ì¼ë°˜ ê³µê²©(-1) ì„ íƒ ë¹„ìœ¨: {neg_strategy_ratio:.2%}")
    
    return overall_win_rate, win_rates.get("black", 0), win_rates.get("white", 0), neg_strategy_ratio

class VsModelCEnv(gym.Env):
    """
    ë‹¨ì¼ PPO ì—ì´ì „íŠ¸ê°€ ê³ ì • ìƒëŒ€(Model C)ì™€ ë²ˆê°ˆì•„ ì‹¸ìš°ë©° í•™ìŠµí•˜ë„ë¡ ë˜í•‘í•œ í™˜ê²½.
    í•œ ë²ˆì˜ step() í˜¸ì¶œì—ì„œ:
      - PPO(ì—ì´ì „íŠ¸)ì˜ ìˆ˜ë¥¼ env.step(action)ìœ¼ë¡œ ë°˜ì˜
      - ê²Œì„ ë¯¸ì¢…ë£Œë©´, ê³§ë°”ë¡œ Cì˜ ìˆ˜ë¥¼ ë‚´ë¶€ì—ì„œ ì‹¤í–‰
      - ë‹¤ì‹œ PPO ì°¨ë¡€ê°€ ëœ ì‹œì ì˜ ê´€ì¸¡ obs, reward(ì—ì´ì „íŠ¸ ê´€ì ), done ë“±ì„ ë°˜í™˜
    """
    metadata = {"render_modes": []}

    def __init__(self, agent_side="black"):
        super().__init__()
        self.base_env = AlggaGoEnv()  # ê¸°ì¡´ í™˜ê²½ ì¬ì‚¬ìš©
        self.agent_side = agent_side  # 'black' or 'white'
        self.action_space = self.base_env.action_space
        self.observation_space = self.base_env.observation_space

        # ë‚´ë¶€ ìƒíƒœ ì¶”ì ìš©
        self._last_obs = None

    def reset(self, *, seed=None, options=None):
        # PPOê°€ í•­ìƒ ë¨¼ì € ë‘ë„ë¡ ì‹œì‘ ìƒ‰ì„ ê°•ì œ
        initial_player = self.agent_side
        self._last_obs, info = self.base_env.reset(options={"initial_player": initial_player})
        # í˜¹ì‹œ ì‹œì‘ í”Œë ˆì´ì–´ê°€ PPOê°€ ì•„ë‹Œ ê²½ìš°ì—”, Cê°€ ë¨¼ì € í•œ ìˆ˜ ë‘ê³  PPO ì°¨ë¡€ë¡œ ë§ì¶°ì¤Œ
        if self.base_env.current_player != self.agent_side:
            self._play_model_c_turn()
            self._last_obs = self.base_env._get_obs()
        return self._last_obs, info

    def step(self, action):
        """
        ì…ë ¥ actionì€ í•­ìƒ PPO(ì—ì´ì „íŠ¸) ì°¨ë¡€ì˜ í–‰ë™ìœ¼ë¡œ ê°€ì •.
        ì´í›„ Cì˜ ì°¨ë¡€ê¹Œì§€ ë‚´ë¶€ì—ì„œ ì§„í–‰í•˜ì—¬, ë‹¤ì‹œ PPO ì°¨ë¡€ê°€ ë˜ì—ˆì„ ë•Œì˜ obsë¥¼ ë°˜í™˜.
        """
        # --- 1) PPO(ì—ì´ì „íŠ¸) ì°¨ë¡€: ê¸°ì¡´ env.step ì‚¬ìš© ---
        obs, reward_agent, terminated, truncated, info = self.base_env.step(action)

        # ì—í”¼ì†Œë“œê°€ ëë‚¬ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì¢…ë£Œ
        if terminated or truncated:
            return obs, reward_agent, terminated, truncated, info

        # --- 2) Model C ì°¨ë¡€: ê¸°ì¡´ í‰ê°€ ë¡œì§ì„ ì°¸ê³ í•˜ì—¬ ì§ì ‘ 1ìˆ˜ ì§„í–‰ ---
        # (evaluate_vs_model_c / visualize_vs_model_cì™€ ë™ì¼ ë¡œì§ì„ ì¶•ì•½ ì ìš©):contentReference[oaicite:1]{index=1}
        self._play_model_c_turn()

        # --- 3) ì¢…ë£Œ ì²´í¬ ë° ê´€ì¸¡ ë°˜í™˜(ì´ì œ ë‹¤ì‹œ PPO ì°¨ë¡€) ---
        # Cì˜ ìˆ˜ë¡œ ëë‚¬ë‹¤ë©´ ì—ì´ì „íŠ¸ ê´€ì ì—ì„œ íŒ¨ë°° í˜ë„í‹°ë¥¼ ì•½ê°„ ë”í•´ë„ ë¨
        # (env.stepì—ì„œ ìŠ¹/íŒ¨ ë³´ìƒ ì¼ë¶€ê°€ ì—ì´ì „íŠ¸ í„´ì—ë§Œ ë°˜ì˜ë˜ë¯€ë¡œ ë³´ì •)
        terminated_now, loser_penalty = self._check_terminal_and_penalty_after_c_turn()
        total_reward = reward_agent + loser_penalty
        self._last_obs = self.base_env._get_obs()
        return self._last_obs, total_reward, terminated_now, False, info

    # ===== ë‚´ë¶€ ìœ í‹¸ =====
    def _play_model_c_turn(self):
        # í˜„ì¬ ì°¨ë¡€ê°€ Cì¸ì§€ í™•ì¸
        current_player_color = self.base_env.current_player
        if current_player_color == self.agent_side:
            return  # ì´ë¯¸ PPO ì°¨ë¡€ë©´ ì•„ë¬´ê²ƒë„ ì•ˆ í•¨

        action_tuple = model_c_action(self.base_env.stones, current_player_color)
        if action_tuple:
            from pymunk import Vec2d
            from physics import scale_force, all_stones_stopped, WIDTH, HEIGHT, MARGIN
            idx, angle, force = action_tuple

            player_color_tuple = (0, 0, 0) if current_player_color == "black" else (255, 255, 255)
            player_stones = [s for s in self.base_env.stones if s.color[:3] == player_color_tuple]
            if 0 <= idx < len(player_stones):
                stone_to_shoot = player_stones[idx]
                direction = Vec2d(1, 0).rotated(angle)
                impulse = direction * scale_force(force)
                stone_to_shoot.body.apply_impulse_at_world_point(impulse, stone_to_shoot.body.position)

            # ë¬¼ë¦¬ ì§„í–‰ (í‰ê°€ ì½”ë“œì™€ ë™ì¼ ìƒí•œ):contentReference[oaicite:2]{index=2}
            from physics import all_stones_stopped, WIDTH, HEIGHT, MARGIN
            physics_steps = 0
            while not all_stones_stopped(self.base_env.stones) and physics_steps < 600:
                self.base_env.space.step(1/60.0)
                physics_steps += 1

            # ë°”ê¹¥ìœ¼ë¡œ ë‚˜ê°„ ëŒ ì œê±°
            for shape in self.base_env.stones[:]:
                x, y = shape.body.position
                if not (MARGIN < x < WIDTH - MARGIN and MARGIN < y < HEIGHT - MARGIN):
                    if shape in self.base_env.space.shapes:
                        self.base_env.space.remove(shape, shape.body)
                    if shape in self.base_env.stones:
                        self.base_env.stones.remove(shape)

            # í„´ ì „í™˜ ë° ê´€ì¸¡ ì—…ë°ì´íŠ¸
            self.base_env.current_player = "white" if current_player_color == "black" else "black"

    def _check_terminal_and_penalty_after_c_turn(self):
        """
        C ì°¨ë¡€ ì§„í–‰ ì§í›„ ì¢…ë£Œ ì—¬ë¶€ì™€ ì—ì´ì „íŠ¸ ê´€ì  íŒ¨ë„í‹°ë¥¼ ê³„ì‚°.
        env.step ë‚´ë¶€ì˜ ë³´ìƒì€ 'ìˆ˜ë¥¼ ë‘” ìª½' ê¸°ì¤€ìœ¼ë¡œ ì‚°ì¶œë˜ë¯€ë¡œ,
        Cê°€ ì´ê²¨ì„œ ëë‚œ ê²½ìš° ì—ì´ì „íŠ¸ì— ì•½í•œ íŒ¨ë„í‹°ë¥¼ ë”í•´ì¤Œ(-5.0).
        """
        current_black = sum(1 for s in self.base_env.stones if s.color[:3] == (0, 0, 0))
        current_white = sum(1 for s in self.base_env.stones if s.color[:3] == (255, 255, 255))
        if current_black == 0 and current_white > 0:
            # ë°± ìŠ¹(í‘ ì˜¬ì•„ì›ƒ)
            winner = "white"
            terminated = True
        elif current_white == 0 and current_black > 0:
            winner = "black"
            terminated = True
        else:
            return False, 0.0

        # ì—ì´ì „íŠ¸ íŒ¨ë°° ì‹œë§Œ ì‘ì€ íŒ¨ë„í‹°
        agent_color = self.agent_side
        if (winner == "white" and agent_color == "black") or (winner == "black" and agent_color == "white"):
            return True, -5.0
        return True, 0.0

class VsFixedOpponentEnv(gym.Env):
    """
    ë‹¨ì¼ PPO ì—ì´ì „íŠ¸ê°€ 'ê³ ì •ëœ PPO ìƒëŒ€(opponent_model)'ì™€ ë²ˆê°ˆì•„ ì‹¸ìš°ë©° í•™ìŠµí•˜ëŠ” í™˜ê²½.
    """
    metadata = {"render_modes": []}

    def __init__(self, opponent_model: PPO, agent_side="black"):
        super().__init__()
        self.base_env = AlggaGoEnv()
        self.opponent = opponent_model
        self.agent_side = agent_side  # 'black' or 'white'
        self.action_space = self.base_env.action_space
        self.observation_space = self.base_env.observation_space
        self._last_obs = None

    def reset(self, *, seed=None, options=None):
        initial_player = self.agent_side
        self._last_obs, info = self.base_env.reset(options={"initial_player": initial_player})
        # ì‹œì‘ ì°¨ë¡€ê°€ ì—ì´ì „íŠ¸ê°€ ì•„ë‹ˆë©´, ìƒëŒ€ê°€ ë¨¼ì € í•œ ìˆ˜ ë‘ê³  ì—ì´ì „íŠ¸ ì°¨ë¡€ë¡œ ë§ì¶¤
        if self.base_env.current_player != self.agent_side:
            self._play_opponent_turn()
            self._last_obs = self.base_env._get_obs()
        return self._last_obs, info

    def step(self, action):
        # 1) ì—ì´ì „íŠ¸ ìˆ˜
        obs, reward_agent, terminated, truncated, info = self.base_env.step(action)
        if terminated or truncated:
            return obs, reward_agent, terminated, truncated, info

        # 2) ìƒëŒ€ ìˆ˜ (PPO)
        self._play_opponent_turn()

        # 3) ì¢…ë£Œ/íŒ¨ë„í‹° ë³´ì • ë° ë‹¤ìŒ ê´€ì¸¡ ë°˜í™˜ (ë‹¤ì‹œ ì—ì´ì „íŠ¸ ì°¨ë¡€)
        terminated_now, loser_penalty = self._check_terminal_and_penalty_after_opponent()
        total_reward = reward_agent + loser_penalty
        self._last_obs = self.base_env._get_obs()
        return self._last_obs, total_reward, terminated_now, False, info

    # ===== ë‚´ë¶€ ìœ í‹¸ =====
    def _play_opponent_turn(self):
        if self.base_env.current_player == self.agent_side:
            return  # ì§€ê¸ˆì€ ì—ì´ì „íŠ¸ ì°¨ë¡€ë©´ ì•„ë¬´ ê²ƒë„ ì•ˆ í•¨
        # í˜„ì¬ ìƒíƒœ ê´€ì¸¡ìœ¼ë¡œ ìƒëŒ€ ì •ì±… ì‹¤í–‰ â†’ ê·¸ëŒ€ë¡œ env.step
        opp_obs = self.base_env._get_obs()
        opp_action, _ = self.opponent.predict(opp_obs, deterministic=True)
        self.base_env.step(opp_action)  # ë³´ìƒì€ ìƒëŒ€ ê¸°ì¤€ì´ë¯€ë¡œ ì—¬ê¸°ì„  ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

    def _check_terminal_and_penalty_after_opponent(self):
        current_black = sum(1 for s in self.base_env.stones if s.color[:3] == (0, 0, 0))
        current_white = sum(1 for s in self.base_env.stones if s.color[:3] == (255, 255, 255))
        if current_black == 0 and current_white > 0:
            winner = "white"; terminated = True
        elif current_white == 0 and current_black > 0:
            winner = "black"; terminated = True
        else:
            return False, 0.0

        # ì—ì´ì „íŠ¸ê°€ ì§„ ê²½ìš°ë§Œ ì‘ì€ íŒ¨ë„í‹°
        if (winner == "white" and self.agent_side == "black") or \
           (winner == "black" and self.agent_side == "white"):
            return True, -5.0
        return True, 0.0
    
# --- Vs Model C í™˜ê²½ ìƒì„± í—¬í¼ ---
def make_vs_c_env_vec(n_envs: int = 2):
    """
    PPOê°€ Model Cì™€ ë²ˆê°ˆì•„ ì‹¸ìš°ë©° í•™ìŠµí•˜ë„ë¡ í‘/ë°±ì„ ì„ì€ VecEnvë¥¼ ë§Œë“­ë‹ˆë‹¤.
    ì§ìˆ˜ index -> black, í™€ìˆ˜ index -> white
    """
    def _maker(i):
        side = "black" if i % 2 == 0 else "white"
        return lambda: VsModelCEnv(agent_side=side)
    return DummyVecEnv([_maker(i) for i in range(n_envs)])

def make_vs_opponent_env_vec(opponent_model: PPO, n_envs: int = 2):
    """
    ë³‘ë ¬ env ì¤‘ ì§ìˆ˜ indexëŠ” agent=í‘, í™€ìˆ˜ indexëŠ” agent=ë°±ìœ¼ë¡œ ë§Œë“¤ì–´
    í•™ìŠµ ê³¼ì •ì—ì„œ ìƒ‰ìƒì´ ê· í˜•ë˜ë„ë¡ í•¨.
    """
    def _maker(i):
        side = "black" if i % 2 == 0 else "white"
        return lambda: VsFixedOpponentEnv(opponent_model=opponent_model, agent_side=side)
    return DummyVecEnv([_maker(i) for i in range(n_envs)])

def train_vs_model_c(total_timesteps=100_000, agent_side="black", ent_coef=0.1, save_name="ppo_vs_c"):
    """
    PPO í•˜ë‚˜ë¥¼ ê³ ì • ìƒëŒ€(Model C)ì™€ ì‹¸ìš°ë©° í•™ìŠµí•˜ëŠ” ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„.
    """
    os.makedirs(SAVE_DIR, exist_ok=True)
    os.makedirs(LOG_DIR, exist_ok=True)

    # ë˜í¼ í™˜ê²½ ì‚¬ìš©
    def _make():
        return VsModelCEnv(agent_side=agent_side)
    env = DummyVecEnv([_make])

    model = PPO("MlpPolicy", env, verbose=1, ent_coef=ent_coef)
    # (ì„ íƒ) ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ì´ˆê¸°í™” ì¬ì‚¬ìš© ê°€ëŠ¥:contentReference[oaicite:4]{index=4}
    initialize_to_rule_based(model)

    print(f"[INFO] PPO vs Model C í•™ìŠµ ì‹œì‘: total_timesteps={total_timesteps}, side={agent_side}, ent_coef={ent_coef}")
    model.learn(total_timesteps=total_timesteps, callback=ProgressCallback(total_timesteps), reset_num_timesteps=False)

    save_path = os.path.join(SAVE_DIR, f"{save_name}_{agent_side}_{total_timesteps}.zip")
    model.save(save_path)
    print(f"[INFO] í•™ìŠµ ì™„ë£Œ. ì €ì¥: {os.path.basename(save_path)}")
    return model, save_path

# --- ì‹œê°í™” í•¨ìˆ˜ ---
def visualize_one_game(model_A: PPO, model_B: PPO, ent_A: float, ent_B: float, stage_num: int, force_A_as_black: bool = None):
    """
    í•œ ê²Œì„ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    force_A_as_black: Trueì´ë©´ Aê°€ í‘ëŒ, Falseì´ë©´ Bê°€ í‘ëŒ, Noneì´ë©´ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.
    """
    stage_str = f"ìŠ¤í…Œì´ì§€ {stage_num}" if stage_num > 0 else "ì´ˆê¸° ìƒíƒœ"
    
    # í‘ëŒ/ë°±ëŒ ëª¨ë¸ ë° ìº¡ì…˜ ê²°ì •
    if force_A_as_black is True:
        black_model, white_model = model_A, model_B
        caption = f"{stage_str} Eval: A(Black, ent={ent_A:.3f}) vs B(White, ent={ent_B:.3f})"
    elif force_A_as_black is False:
        black_model, white_model = model_B, model_A
        caption = f"{stage_str} Eval: B(Black, ent={ent_B:.3f}) vs A(White, ent={ent_A:.3f})"
    else: # ê¸°ë³¸ ë™ì‘ (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
        if ent_A >= ent_B:
            black_model, white_model = model_A, model_B
            caption = f"{stage_str} Eval: A(Black, ent={ent_A:.3f}) vs B(White, ent={ent_B:.3f})"
        else:
            black_model, white_model = model_B, model_A
            caption = f"{stage_str} Eval: B(Black, ent={ent_B:.3f}) vs A(White, ent={ent_A:.3f})"

    print(f"\n--- ì‹œê°í™” í‰ê°€: {caption} ---")
    
    env = AlggaGoEnv()
    obs, _ = env.reset(options={"initial_player": "black"})
    pygame.init()
    screen = pygame.display.set_mode((WIDTH, HEIGHT))
    pygame.display.set_caption(caption)
    from physics import scale_force
    from pymunk import Vec2d
    from opponent import get_regular_action, get_split_shot_action

    done = False; step_count = 0; max_steps = 200; info = {}
    while not done and step_count < max_steps:
        for event in pygame.event.get():
            if event.type == pygame.QUIT: done = True
        if done: continue
        current_player = env.current_player
        action_model = black_model if current_player == "black" else white_model

        env.render(screen=screen)
        pygame.display.flip()
        time.sleep(0.5)

        action_values, _ = action_model.predict(obs.reshape(1, -1), deterministic=True)
        strategy_choice, index_weight, angle_offset, force_offset = np.squeeze(action_values)

        player_stones = [s for s in env.stones if s.color[:3] == ((0,0,0) if current_player == "black" else (255,255,255))]
        opponent_stones = [s for s in env.stones if s.color[:3] != ((0,0,0) if current_player == "black" else (255,255,255))]

        if not player_stones or not opponent_stones: break

        if strategy_choice >= 0:
            rule_action = get_split_shot_action(player_stones, opponent_stones)
        else:
            rule_action = get_regular_action(player_stones, opponent_stones)
        
        if rule_action is None:
            rule_action = get_regular_action(player_stones, opponent_stones)
        
        if rule_action is None: break

        rule_idx, rule_angle, rule_force = rule_action
        
        if len(player_stones) > 1:
            idx_offset = int(np.round(index_weight * (len(player_stones) - 1) / 2))
            final_idx = np.clip(rule_idx + idx_offset, 0, len(player_stones) - 1)
        else: final_idx = 0
        
        final_angle = rule_angle + angle_offset
        final_force = np.clip(rule_force + force_offset, 0.0, 1.0)
        
        selected_stone = player_stones[final_idx]
        direction = Vec2d(1, 0).rotated(final_angle)
        impulse = direction * scale_force(final_force)
        
        selected_stone.body.apply_impulse_at_world_point(impulse, selected_stone.body.position)
        
        physics_steps = 0
        while not all_stones_stopped(env.stones) and physics_steps < 300:
            env.space.step(1/60.0)
            env.render(screen=screen)
            pygame.display.flip()
            time.sleep(1/60.0)
            physics_steps += 1
        
        current_black = sum(1 for s in env.stones if s.color[:3] == (0,0,0))
        current_white = sum(1 for s in env.stones if s.color[:3] == (255,255,255))
        if current_black == 0: done = True; info['winner'] = 'white'
        elif current_white == 0: done = True; info['winner'] = 'black'
        
        env.current_player = "white" if current_player == "black" else "black"
        obs = env._get_obs()
        step_count += 1
        
    winner = info.get('winner', 'Draw/Timeout')
    print(f">>> ì‹œê°í™” ì¢…ë£Œ: ìµœì¢… ìŠ¹ì {winner} <<<")
    time.sleep(2)
    pygame.quit()

def visualize_vs_model_c(ppo_model: PPO, round_num: int, ppo_player_side: str):
    """
    PPO ëª¨ë¸ê³¼ ëª¨ë¸ Cì˜ ëŒ€ê²°ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    ppo_player_side: PPO ëª¨ë¸ì´ í”Œë ˆì´í•  ìƒ‰ìƒ ('black' ë˜ëŠ” 'white')
    """
    stage_str = f"íŠ¹ë³„ í›ˆë ¨ {round_num}ë¼ìš´ë“œ"
    
    if ppo_player_side == "black":
        caption = f"{stage_str}: ëª¨ë¸ A(í‘ëŒ) vs ëª¨ë¸ C(ë°±ëŒ)"
    else:
        caption = f"{stage_str}: ëª¨ë¸ C(í‘ëŒ) vs ëª¨ë¸ A(ë°±ëŒ)"

    print(f"\n--- {stage_str} ì‹œê°í™” ({caption}) ---")

    env = AlggaGoEnv()
    # ê²Œì„ì€ í•­ìƒ í‘ëŒë¶€í„° ì‹œì‘í•˜ë„ë¡ ê³ ì •í•©ë‹ˆë‹¤.
    obs, _ = env.reset(options={"initial_player": "black"})
    
    pygame.init()
    screen = pygame.display.set_mode((WIDTH, HEIGHT))
    pygame.display.set_caption(caption)
    
    from pymunk import Vec2d
    from physics import scale_force, all_stones_stopped
    from opponent import get_regular_action, get_split_shot_action

    done = False
    info = {}
    while not done:
        for event in pygame.event.get():
            if event.type == pygame.QUIT: done = True
        if done: continue

        current_player_color = env.current_player
        
        env.render(screen=screen)
        pygame.display.flip()
        time.sleep(0.5)

        action_tuple = None
        # í˜„ì¬ í„´ í”Œë ˆì´ì–´ê°€ PPO ëª¨ë¸ì´ í”Œë ˆì´í•´ì•¼ í•˜ëŠ” ìƒ‰ìƒê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        if current_player_color == ppo_player_side: # PPO ëª¨ë¸ì˜ í„´
            action_values, _ = ppo_model.predict(obs, deterministic=True)
            strategy_choice, index_weight, angle_offset, force_offset = np.squeeze(action_values)

            player_color_tuple = (0,0,0) if ppo_player_side == "black" else (255,255,255)
            opponent_color_tuple = (255,255,255) if ppo_player_side == "black" else (0,0,0)

            player_stones = [s for s in env.stones if s.color[:3] == player_color_tuple]
            opponent_stones = [s for s in env.stones if s.color[:3] == opponent_color_tuple]

            if not player_stones or not opponent_stones: break
            
            if strategy_choice >= 0:
                rule_action = get_split_shot_action(player_stones, opponent_stones)
            else:
                rule_action = get_regular_action(player_stones, opponent_stones)

            if rule_action is None:
                rule_action = get_regular_action(player_stones, opponent_stones)

            if rule_action:
                rule_idx, rule_angle, rule_force = rule_action
                if len(player_stones) > 1:
                    idx_offset = int(np.round(index_weight * (len(player_stones) - 1) / 2))
                    final_idx = np.clip(rule_idx + idx_offset, 0, len(player_stones) - 1)
                else: final_idx = 0
                
                final_angle = rule_angle + angle_offset
                final_force = np.clip(rule_force + force_offset, 0.0, 1.0)
                action_tuple = (final_idx, final_angle, final_force)

        else: # ëª¨ë¸ Cì˜ í„´
            action_tuple = model_c_action(env.stones, current_player_color)

        if action_tuple:
            idx, angle, force = action_tuple
            player_color_tuple = (0,0,0) if current_player_color == "black" else (255,255,255)
            player_stones = [s for s in env.stones if s.color[:3] == player_color_tuple]
            
            if 0 <= idx < len(player_stones):
                stone_to_shoot = player_stones[idx]
                direction = Vec2d(1, 0).rotated(angle)
                impulse = direction * scale_force(force)
                stone_to_shoot.body.apply_impulse_at_world_point(impulse, stone_to_shoot.body.position)
        
        physics_steps = 0
        while not all_stones_stopped(env.stones) and physics_steps < 600:
            env.space.step(1/60.0)
            env.render(screen=screen)
            pygame.display.flip()
            time.sleep(1/60.0)
            physics_steps += 1
        
        for shape in env.stones[:]:
            if not (MARGIN < shape.body.position.x < WIDTH - MARGIN and MARGIN < shape.body.position.y < HEIGHT - MARGIN):
                if shape in env.space.shapes:
                    env.space.remove(shape, shape.body)
                if shape in env.stones:
                    env.stones.remove(shape)
        
        current_black = sum(1 for s in env.stones if s.color[:3] == (0,0,0))
        current_white = sum(1 for s in env.stones if s.color[:3] == (255,255,255))
        if current_black == 0:
            done = True; info['winner'] = 'white'
        elif current_white == 0:
            done = True; info['winner'] = 'black'
        
        env.current_player = "white" if current_player_color == "black" else "black"
        obs = env._get_obs()

    winner = info.get('winner', 'Draw/Timeout')
    print(f">>> ì‹œê°í™” ì¢…ë£Œ: ìµœì¢… ìŠ¹ì {winner} <<<")
    time.sleep(2)
    pygame.quit()

def run_gauntlet_training(model_to_train, model_name, initial_timesteps):
    """
    ì£¼ì–´ì§„ ëª¨ë¸ì´ ëª¨ë¸ Cë¥¼ ì´ê¸¸ ë•Œê¹Œì§€ í›ˆë ¨í•˜ëŠ” ì˜ˆì„ ì „ í•¨ìˆ˜.
    """
    print("\n" + "="*50)
    print(f"ğŸ¥Š      íŠ¹ë³„ ì˜ˆì„  ì‹œì‘: ëª¨ë¸ {model_name} vs ëª¨ë¸ C       ğŸ¥Š")
    print("="*50)

    GAUNTLET_LOG_FILE = os.path.join(LOG_DIR, "gauntlet_log.csv")
    GAUNTLET_SAVE_PATH = os.path.join(SAVE_DIR, f"model_{model_name.lower()}_gauntlet_in_progress.zip")

    if os.path.exists(GAUNTLET_SAVE_PATH):
        print(f"\n[INFO] ì§„í–‰ ì¤‘ì´ë˜ ì˜ˆì„ ì „ ëª¨ë¸({os.path.basename(GAUNTLET_SAVE_PATH)})ì„ ë¡œë“œí•˜ì—¬ ì´ì–´ê°‘ë‹ˆë‹¤.")
        # í˜„ì¬ envì™€ device ì„¤ì •ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        model_to_train = PPO.load(GAUNTLET_SAVE_PATH, env=model_to_train.get_env(), device=model_to_train.device)
        initial_timesteps = model_to_train.num_timesteps
        print(f"[INFO] ë¡œë“œëœ ëª¨ë¸ì˜ ëˆ„ì  íƒ€ì„ìŠ¤í…: {initial_timesteps:,}")
    else:
        print(f"\n[INFO] ëª¨ë¸ {model_name}ì— ëŒ€í•œ ìƒˆ ì˜ˆì„ ì „ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    original_ent_coef = model_to_train.ent_coef
    model_to_train.ent_coef = 0.0 # í‰ê°€ ì‹œì—ëŠ” ì—”íŠ¸ë¡œí”¼ 0ìœ¼ë¡œ ê³ ì •
    print(f"\n[INFO] ì˜ˆì„  í‰ê°€ë¥¼ ìœ„í•´ ëª¨ë¸ {model_name}ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")

    GAUNTLET_TIMESTEPS = 50000
    GAUNTLET_EVAL_EPISODES_PER_COLOR = 100
    N_ENVS_VS_C = 2  # í‘/ë°± ë²ˆê°ˆì•„

    # í•„ìš”ì‹œ ì›ë˜ envë¥¼ ë³µì›í•  ìˆ˜ ìˆë„ë¡ ë°±ì—…(ì„ íƒ)
    original_env = getattr(model_to_train, "env", None)
    
    gauntlet_round = 1
    current_total_timesteps = initial_timesteps

    while True:
        # [ìˆ˜ì •] í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” 3ê°œì˜ ê°’ì„ ê°ê°ì˜ ë³€ìˆ˜ë¡œ ë°›ë„ë¡ ìˆ˜ì •
        overall_win_rate, win_rate_as_black, win_rate_as_white, neg_strategy_ratio = evaluate_vs_model_c(model_to_train, num_episodes_per_color=GAUNTLET_EVAL_EPISODES_PER_COLOR)
        
        with open(GAUNTLET_LOG_FILE, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([f"{model_name}_Round_{gauntlet_round}", current_total_timesteps, f"{win_rate_as_black:.4f}", f"{win_rate_as_white:.4f}", f"{overall_win_rate:.4f}", f"{neg_strategy_ratio:.4f}"])
        print("   [INFO] ì˜ˆì„ ì „ ê²°ê³¼ê°€ CSV ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # --- ë“€ì–¼ ì‹œê°í™” í‰ê°€ (vs Model C) ---
        print("\n[INFO] ì˜ˆì„ ì „ ì‹œê°í™” í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (í‘ëŒ/ë°±ëŒ ê°ê° 1íšŒ).")
        # 1. PPO ëª¨ë¸ì´ í‘ëŒì¼ ë•Œ
        visualize_vs_model_c(model_to_train, round_num=gauntlet_round, ppo_player_side="black")
        # 2. PPO ëª¨ë¸ì´ ë°±ëŒì¼ ë•Œ
        visualize_vs_model_c(model_to_train, round_num=gauntlet_round, ppo_player_side="white")

        if overall_win_rate > 0.5:
            print(f"\nğŸ† ëª¨ë¸ {model_name}ì´(ê°€) ëª¨ë¸ Cë¥¼ ìƒëŒ€ë¡œ ìŠ¹ë¦¬í–ˆìŠµë‹ˆë‹¤! ì˜ˆì„ ì„ í†µê³¼í•©ë‹ˆë‹¤. ğŸ†")
            model_to_train.ent_coef = original_ent_coef
            # (ì„ íƒ) ì›ë˜ env ë³µì›
            if original_env is not None:
                model_to_train.set_env(original_env)
            if os.path.exists(GAUNTLET_SAVE_PATH):
                os.remove(GAUNTLET_SAVE_PATH)
            break
        else:
            print(f"   - ì „ì²´ ìŠ¹ë¥ ({overall_win_rate:.2%})ì´ 50% ë¯¸ë§Œì…ë‹ˆë‹¤. ëª¨ë¸ {model_name}ì„(ë¥¼) **Model Cì™€ ì‹¸ìš°ë©°** ì¶”ê°€ í›ˆë ¨í•©ë‹ˆë‹¤.")
            # í•™ìŠµ ë•ŒëŠ” ì—”íŠ¸ë¡œí”¼ ë³µêµ¬
            model_to_train.ent_coef = original_ent_coef

            # âœ… ì—¬ê¸°! í•™ìŠµ í™˜ê²½ì„ VsModelCEnvë¡œ ë°”ê¿‰ë‹ˆë‹¤.
            train_env = make_vs_c_env_vec(n_envs=N_ENVS_VS_C)
            model_to_train = reload_with_env(model_to_train, train_env)

            model_to_train.learn(
                total_timesteps=GAUNTLET_TIMESTEPS,
                callback=ProgressCallback(GAUNTLET_TIMESTEPS),
                reset_num_timesteps=False
            )
            current_total_timesteps = model_to_train.num_timesteps
            model_to_train.save(GAUNTLET_SAVE_PATH)
            print(f" ğŸ’¾  ì˜ˆì„  í›ˆë ¨ ì§„í–‰ ìƒí™©ì„ {os.path.basename(GAUNTLET_SAVE_PATH)} íŒŒì¼ì— ë®ì–´ì¼ìŠµë‹ˆë‹¤.")
            model_to_train.ent_coef = 0.0 # ë‹¤ìŒ í‰ê°€ë¥¼ ìœ„í•´ ë‹¤ì‹œ 0ìœ¼ë¡œ ì„¤ì •
        gauntlet_round += 1
    
    return model_to_train, current_total_timesteps

def reload_with_env(model: PPO, new_env):
    """í˜„ì¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ë³´ì¡´í•œ ì±„, env ê°œìˆ˜ë¥¼ ë°”ê¾¸ê¸° ìœ„í•´ ì €ì¥-ì¬ë¡œë”©."""
    tmp = os.path.join(SAVE_DIR, "_tmp_reload_swap_env.zip")
    os.makedirs(SAVE_DIR, exist_ok=True)
    model.save(tmp)
    new_model = PPO.load(tmp, env=new_env, device=model.device)
    try:
        os.remove(tmp)
    except OSError:
        pass
    return new_model

def run_final_evaluation(champion_model: PPO, env):
    """
    ìµœì¢… ì±”í”¼ì–¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì—¬ëŸ¬ ì—”íŠ¸ë¡œí”¼ ë ˆë²¨ì˜ ìƒëŒ€ì™€ ë¹„êµí•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*35)
    print("ğŸ†      ìµœì¢… ì±”í”¼ì–¸ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€      ğŸ†")
    print("="*35 + "\n")
    
    opponent_ent_coefs = [0.0, 0.1, 0.2, 0.3, 0.4]
    results = []
    
    # ì±”í”¼ì–¸ ëª¨ë¸ì„ ì„ì‹œ ì €ì¥í•˜ì—¬ ê¹¨ë—í•œ ìƒíƒœì˜ ìƒëŒ€ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•¨
    champion_path = os.path.join(SAVE_DIR, "champion_model_final.zip")
    champion_model.save(champion_path)

    print(f"[*] ì±”í”¼ì–¸ ëª¨ë¸: {os.path.basename(champion_model.logger.get_dir())} ì—ì„œ ì €ì¥ë¨")
    print("[*] í‰ê°€ ìƒëŒ€: ì±”í”¼ì–¸ê³¼ ë™ì¼í•œ ëª¨ë¸ (ì—”íŠ¸ë¡œí”¼ë§Œ 0.0 ~ 0.4ë¡œ ê³ ì •)")
    print("-" * 35)

    for ent_coef in opponent_ent_coefs:
        print(f"\n[í‰ê°€] vs Opponent (ent_coef: {ent_coef:.1f})")
        
        # ì±”í”¼ì–¸ì˜ ë³µì‚¬ë³¸ì„ ìƒëŒ€ë¡œ ë¡œë“œ
        opponent_model = PPO.load(champion_path, env=env)
        opponent_model.ent_coef = ent_coef
        
        # 100íŒì”© ê³µì • í‰ê°€ ì§„í–‰
        win_rate_champion, _, _, _ = evaluate_fairly(
            champion_model, opponent_model, num_episodes=100
        )
        
        results.append((ent_coef, win_rate_champion))
        print(f"  â–¶ ì±”í”¼ì–¸ ìŠ¹ë¥ : {win_rate_champion:.2%}")

    print("\n\n" + "="*30)
    print("      ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼ ìš”ì•½ ğŸ“Š")
    print("="*30)
    print(f"{'ìƒëŒ€ ì—”íŠ¸ë¡œí”¼':<15} | {'ì±”í”¼ì–¸ ìŠ¹ë¥ ':<15}")
    print("-" * 30)
    for ent_coef, win_rate in results:
        print(f"{ent_coef:<15.1f} | {win_rate:<15.2%}")
    print("="*30)
    
    # ì„ì‹œ ì €ì¥ëœ ì±”í”¼ì–¸ ëª¨ë¸ ì‚­ì œ
    if os.path.exists(champion_path):
        os.remove(champion_path)

# --- ê²½ìŸì  í•™ìŠµ ë©”ì¸ í•¨ìˆ˜ ---
def main():
    os.makedirs(SAVE_DIR, exist_ok=True)
    os.makedirs(LOG_DIR, exist_ok=True)
    temp_env = DummyVecEnv([make_env_fn()])

    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    MAIN_LOG_FILE = os.path.join(LOG_DIR, "training_log.csv")
    GAUNTLET_LOG_FILE = os.path.join(LOG_DIR, "gauntlet_log.csv")

    # ë¡œê·¸ íŒŒì¼ í—¤ë” ì´ˆê¸°í™”
    if not os.path.exists(MAIN_LOG_FILE):
        with open(MAIN_LOG_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Stage", "Total Timesteps", "Model A Entropy", "Model B Entropy", "Round 1 Win Rate (A Black)", "Round 2 Win Rate (B Black)"])
    if not os.path.exists(GAUNTLET_LOG_FILE):
        with open(GAUNTLET_LOG_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Round", "Total Timesteps", "Win Rate as Black", "Win Rate as White", "Overall Win Rate", "Neg Strategy Ratio"])
    # ìƒíƒœ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ì‹œì‘
    state = load_training_state() or {}
    total_timesteps_so_far = state.get("total_timesteps_so_far", 0)
    current_ent_coef_A = state.get("current_ent_coef_A", INITIAL_ENT_COEF_A)
    current_ent_coef_B = state.get("current_ent_coef_B", INITIAL_ENT_COEF_B)
    best_overall_models = state.get("best_overall_models", [])
    model_A, model_B = None, None
    model_pattern = re.compile(r"model_(a|b)_(\d+)_([0-9.]+)\.zip")

    try:
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
        models_found = {"a": [], "b": []}
        if os.path.exists(SAVE_DIR):
            for f in os.listdir(SAVE_DIR):
                match = model_pattern.match(f)
                if match: models_found[match.group(1)].append((int(match.group(2)), os.path.join(SAVE_DIR, f)))
        if not models_found["a"]: raise FileNotFoundError("í•™ìŠµëœ A ëª¨ë¸ ì—†ìŒ")
        latest_a_path = max(models_found["a"], key=lambda i: i[0])[1]
        latest_b_path = max(models_found["b"], key=lambda i: i[0])[1] if models_found["b"] else latest_a_path
        print(f"[INFO] í•™ìŠµ ì´ì–´í•˜ê¸°: Model A({os.path.basename(latest_a_path)}), Model B({os.path.basename(latest_b_path)}) ë¡œë“œ")
        model_A = PPO.load(latest_a_path, env=temp_env)
        model_B = PPO.load(latest_b_path, env=temp_env)
    except Exception as e:
        # [ìˆ˜ì •] ì›ë˜ì˜ ì •ìƒ ì‘ë™í•˜ë˜ ì´ˆê¸°í™” ìˆœì„œë¡œ ë³µì›
        print(f"[INFO] ìƒˆ í•™ìŠµ ì‹œì‘ ({e}).")
        model_A = PPO("MlpPolicy", temp_env, verbose=0, ent_coef=INITIAL_ENT_COEF_A, max_grad_norm=0.5)
        print("[INFO] ëª¨ë¸ì„ Rule-based ì •ì±…ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        initialize_to_rule_based(model_A)
        print("[INFO] ì •ì±…ì„ rule-based í˜•íƒœë¡œ ê°•ì œ ì´ˆê¸°í™” ì™„ë£Œ")
        try:
            params = model_A.get_parameters()
            params['policy']['action_net.weight'].data.fill_(0)
            params['policy']['action_net.bias'].data.fill_(0)
            model_A.set_parameters(params)
            print("[INFO] ì¶”ê°€ ì´ˆê¸°í™”(action_net->0) ì„±ê³µ.")
        except KeyError:
            print("[ê²½ê³ ] ëª¨ë¸ êµ¬ì¡°ë¥¼ ì°¾ì§€ ëª»í•´ ì¶”ê°€ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print_model_parameters(model_A)
        
        total_timesteps_so_far = 0

        # ì´ˆê¸° ì˜ˆì„ ì „ì€ Aëª¨ë¸ë§Œ ì§„í–‰
        model_A, total_timesteps_so_far = run_gauntlet_training(model_A, "A", total_timesteps_so_far)
        
        # ì˜ˆì„  í†µê³¼í•œ Aë¥¼ ë‹¤ì‹œ ì €ì¥í•˜ê³  Bë¥¼ ê·¸ê²ƒìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ì„ íƒì‚¬í•­, ë” ê³µì •í•œ ì‹œì‘ì„ ìœ„í•´)
        print("\n[INFO] ì˜ˆì„ ì„ í†µê³¼í•œ ëª¨ë¸ Aë¥¼ ë³µì œí•˜ì—¬ ëª¨ë¸ Bë¥¼ ë‹¤ì‹œ ë™ê¸°í™”í•©ë‹ˆë‹¤...")
        post_gauntlet_a_path = os.path.join(SAVE_DIR, "model_a_post_gauntlet.zip")
        model_A.save(post_gauntlet_a_path)
        model_B = PPO.load(post_gauntlet_a_path, env=temp_env)
        print("[INFO] ëª¨ë¸ B ë™ê¸°í™” ì™„ë£Œ.")


    # --- ë©”ì¸ í•™ìŠµ ë£¨í”„ ---
    start_stage = total_timesteps_so_far // TIMESTEPS_PER_STAGE if TIMESTEPS_PER_STAGE > 0 else 0
    total_expected_timesteps = MAX_STAGES * TIMESTEPS_PER_STAGE

    for stage_idx in range(start_stage, MAX_STAGES):
        if total_timesteps_so_far >= total_expected_timesteps: break
        print_overall_progress(stage_idx + 1, MAX_STAGES, total_timesteps_so_far, total_expected_timesteps)
        print(f"\n--- ìŠ¤í…Œì´ì§€ {stage_idx + 1}/{MAX_STAGES} ì‹œì‘ ---")
        stage_start_time = time.time()
        
        current_training_model_name = "A" if current_ent_coef_A >= current_ent_coef_B else "B"
        model_to_train, ent_coef_train = (model_A, current_ent_coef_A) if current_training_model_name == "A" else (model_B, current_ent_coef_B)
        
        model_to_train.ent_coef = ent_coef_train
        opponent_model = model_B if current_training_model_name == "A" else model_A
        train_env = make_vs_opponent_env_vec(opponent_model=opponent_model, n_envs=2)
        model_to_train = reload_with_env(model_to_train, train_env)
        print(f"   í•™ìŠµ ëŒ€ìƒ: Model {current_training_model_name} (ent_coef: {ent_coef_train:.5f})")
        
        model_to_train.learn(total_timesteps=TIMESTEPS_PER_STAGE, callback=ProgressCallback(TIMESTEPS_PER_STAGE), reset_num_timesteps=False)
        total_timesteps_so_far = model_to_train.num_timesteps
        if current_training_model_name == "A": model_A = model_to_train
        else: model_B = model_to_train

        print(f"\n   --- ê²½ìŸ í‰ê°€ ì‹œì‘ ---")
        win_rate_A, win_rate_B, r1_win_rate, r2_win_rate = evaluate_fairly(model_A, model_B, num_episodes=EVAL_EPISODES_FOR_COMPETITION)
        
        with open(MAIN_LOG_FILE, 'a', newline='') as f:
            writer = csv.writer(f)
            log_data = [stage_idx + 1, total_timesteps_so_far, f"{current_ent_coef_A:.5f}", f"{current_ent_coef_B:.5f}", f"{r1_win_rate:.4f}", f"{r2_win_rate:.4f}"]
            writer.writerow(log_data)
        print("   [INFO] í•™ìŠµ ê²°ê³¼ê°€ CSV ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # --- ë“€ì–¼ ì‹œê°í™” í‰ê°€ ---
        # ì´ë²ˆ ìŠ¤í…Œì´ì§€ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì´ Aì¸ì§€ Bì¸ì§€ ê²°ì •
        trained_model_is_A = True if current_training_model_name == "A" else False

        # 1. í•™ìŠµ ëª¨ë¸ì´ í‘ëŒë¡œ í”Œë ˆì´
        print("\n[INFO] ì‹œê°í™” (1/2): í•™ìŠµ ëª¨ë¸ì´ í‘ëŒì¼ ë•Œ í”Œë ˆì´")
        visualize_one_game(model_A, model_B, current_ent_coef_A, current_ent_coef_B, stage_idx + 1, force_A_as_black=trained_model_is_A)

        # 2. í•™ìŠµ ëª¨ë¸ì´ ë°±ëŒë¡œ í”Œë ˆì´
        print("\n[INFO] ì‹œê°í™” (2/2): í•™ìŠµ ëª¨ë¸ì´ ë°±ëŒì¼ ë•Œ í”Œë ˆì´")
        visualize_one_game(model_A, model_B, current_ent_coef_A, current_ent_coef_B, stage_idx + 1, force_A_as_black=(not trained_model_is_A))

        print("\n   --- ì—”íŠ¸ë¡œí”¼ ì¡°ì • ---")
        if win_rate_A > win_rate_B: effective_winner = "A"
        elif win_rate_B > win_rate_A: effective_winner = "B"
        else: effective_winner = "B" if current_training_model_name == "A" else "A"
        
        if win_rate_A == win_rate_B: print(f"   ê²½ìŸ ê²°ê³¼: ë¬´ìŠ¹ë¶€. í•™ìŠµ ëŒ€ìƒ({current_training_model_name})ì´ íŒ¨ë°°í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ Model {effective_winner} ìŠ¹ë¦¬.")
        else: print(f"   ê²½ìŸ ê²°ê³¼: Model {effective_winner} ìŠ¹ë¦¬")

        FINAL_EVAL_ENT_THRESHOLD = 0.45
        should_terminate = False

        model_to_requalify = None
        model_to_requalify_name = ""

        if effective_winner == "A" and current_ent_coef_A > current_ent_coef_B:
            new_ent_coef_B = min(current_ent_coef_B + ENT_COEF_INCREMENT, MAX_ENT_COEF)
            if new_ent_coef_B != current_ent_coef_B:
                print(f"   Model B ì—”íŠ¸ë¡œí”¼ ì¦ê°€ â†’ {new_ent_coef_B:.5f}")
                current_ent_coef_B = new_ent_coef_B
                model_to_requalify = model_B
                model_to_requalify_name = "B"
            if new_ent_coef_B >= FINAL_EVAL_ENT_THRESHOLD:
                run_final_evaluation(champion_model=model_A, env=temp_env)
                should_terminate = True

        elif effective_winner == "B" and current_ent_coef_B > current_ent_coef_A:
            new_ent_coef_A = min(current_ent_coef_A + ENT_COEF_INCREMENT, MAX_ENT_COEF)
            if new_ent_coef_A != current_ent_coef_A:
                print(f"   Model A ì—”íŠ¸ë¡œí”¼ ì¦ê°€ â†’ {new_ent_coef_A:.5f}")
                current_ent_coef_A = new_ent_coef_A
                model_to_requalify = model_A
                model_to_requalify_name = "A"
            if new_ent_coef_A >= FINAL_EVAL_ENT_THRESHOLD:
                run_final_evaluation(champion_model=model_B, env=temp_env)
                should_terminate = True
        else:
            print("   ì—”íŠ¸ë¡œí”¼ ì¡°ì • ì—†ìŒ")

        if model_to_requalify:
            trained_model, total_timesteps_so_far = run_gauntlet_training(model_to_requalify, model_to_requalify_name, total_timesteps_so_far)
            if model_to_requalify_name == "A":
                model_A = trained_model
            else:
                model_B = trained_model

        model_A_path = os.path.join(SAVE_DIR, f"model_a_{total_timesteps_so_far}_{current_ent_coef_A:.3f}.zip")
        model_A.save(model_A_path)
        best_overall_models = update_best_models(best_overall_models, model_A_path, win_rate_A)
        
        model_B_path = os.path.join(SAVE_DIR, f"model_b_{total_timesteps_so_far}_{current_ent_coef_B:.3f}.zip")
        model_B.save(model_B_path)
        
        clean_models(model_A_path, model_B_path, [m[0] for m in best_overall_models])
        
        current_state = {
            "total_timesteps_so_far": total_timesteps_so_far, "current_ent_coef_A": current_ent_coef_A,
            "current_ent_coef_B": current_ent_coef_B, "best_overall_models": best_overall_models
        }
        save_training_state(current_state)
        
        minutes, seconds = divmod(int(time.time() - stage_start_time), 60)
        print(f"\n[ìŠ¤í…Œì´ì§€ {stage_idx + 1}] ì™„ë£Œ (ì†Œìš” ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ)")

        if should_terminate:
            print("\n--- ìµœì¢… í‰ê°€ ì™„ë£Œ. í•™ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ---")
            break

    print("\n--- ì „ì²´ ê²½ìŸì  í•™ìŠµ ì™„ë£Œ ---")
    temp_env.close()
    
if __name__ == "__main__":
    main()